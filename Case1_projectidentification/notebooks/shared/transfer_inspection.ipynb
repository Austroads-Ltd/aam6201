{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.util as util\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mpl_cm\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DATA_DIR\n",
    "\n",
    "train_flattened_mrwa_indices = util.load_data(DATA_DIR / 'processed' / 'MRWA' / \"mrwa_final\" / 'train_flattened_index_mrwa_final.csv')\n",
    "train_flattened_nzta_indices = util.load_data(DATA_DIR / 'processed' / 'NZTA' / \"nzta_final\" / 'train_flattened_index_nzta_final.csv')\n",
    "valid_flattened_mrwa_indices = util.load_data(DATA_DIR / 'processed' / 'MRWA' / \"mrwa_final\" / 'valid_flattened_index_mrwa_final.csv')\n",
    "valid_flattened_nzta_indices = util.load_data(DATA_DIR / 'processed' / 'NZTA' /\"nzta_final\" / 'valid_flattened_index_nzta_final.csv')\n",
    "\n",
    "train_flattened_mrwa_labels = util.load_data(DATA_DIR / 'processed' / 'MRWA' / \"mrwa_final\" / 'train_flattened_labels_mrwa_final.csv', header=[0, 1])\n",
    "train_flattened_nzta_labels = util.load_data(DATA_DIR / 'processed' / 'NZTA' / \"nzta_final\" / 'train_flattened_labels_nzta_final.csv', header=[0, 1])\n",
    "valid_flattened_mrwa_labels = util.load_data(DATA_DIR / 'processed' / 'MRWA' / \"mrwa_final\" / 'valid_flattened_labels_mrwa_final.csv', header=[0, 1])\n",
    "valid_flattened_nzta_labels = util.load_data(DATA_DIR / 'processed' / 'NZTA' / \"nzta_final\" / 'valid_flattened_labels_nzta_final.csv', header=[0, 1])\n",
    "\n",
    "train_flattened_mrwa_truncated = util.load_data(DATA_DIR / 'processed' / 'MRWA' / \"mrwa_final\" / 'train_flattened_data_mrwa_final_no_offset.csv')\n",
    "train_flattened_nzta_truncated = util.load_data(DATA_DIR / 'processed' / 'NZTA' / \"nzta_final\" / 'train_flattened_data_nzta_final_no_offset.csv')\n",
    "valid_flattened_mrwa_truncated = util.load_data(DATA_DIR / 'processed' / 'MRWA' / \"mrwa_final\" / 'valid_flattened_data_mrwa_final_no_offset.csv')\n",
    "valid_flattened_nzta_truncated = util.load_data(DATA_DIR / 'processed' / 'NZTA' / \"nzta_final\" / 'valid_flattened_data_nzta_final_no_offset.csv')\n",
    "\n",
    "train_flattened_vic_truncated = util.load_data(DATA_DIR / 'processed' / 'VIC' / \"final\" / 'train_all.csv')\n",
    "train_flattened_nsw_truncated = util.load_data(DATA_DIR / 'processed' / 'NSW' / \"final\" / 'train_all.csv')\n",
    "valid_flattened_vic_truncated = util.load_data(DATA_DIR / 'processed' / 'VIC' / \"final\" / 'valid_all.csv')\n",
    "valid_flattened_nsw_truncated = util.load_data(DATA_DIR / 'processed' / 'NSW' / \"final\" / 'valid_all.csv')\n",
    "\n",
    "train_flattened_vic_labels = util.load_data(DATA_DIR / 'processed' / 'VIC' / \"final\" / 'labels_all.csv', header=[0, 1])\n",
    "train_flattened_nsw_labels = util.load_data(DATA_DIR / 'processed' / 'NSW' / \"final\" / 'labels_all.csv', header=[0, 1])\n",
    "valid_flattened_vic_labels = util.load_data(DATA_DIR / 'processed' / 'VIC' / \"final\" / 'valid_labels_all.csv', header=[0, 1])\n",
    "valid_flattened_nsw_labels = util.load_data(DATA_DIR / 'processed' / 'NSW' / \"final\" / 'valid_labels_all.csv', header=[0, 1])\n",
    "\n",
    "save_fig_dir = DATA_DIR.parent / 'reports' / 'figures' / 'final_transfer'\n",
    "if not save_fig_dir.exists():\n",
    "    save_fig_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DATA_DIR\n",
    "\n",
    "report_dir = DATA_DIR.parent / 'reports' / 'raw_results'\n",
    "model_dir = DATA_DIR.parent / 'models' / 'trained'\n",
    "\n",
    "# Distribution of train labels vs predicted labels on transfer vs predicted labels on valid\n",
    "from data import DATA_DIR\n",
    "import pickle\n",
    "\n",
    "juris = ['NSW', 'MRWA', 'NZTA']\n",
    "prefixes = ['final'] * 3\n",
    "suffixes = ['even_split'] * 3\n",
    "\n",
    "model_dict = {\n",
    "    juri.replace('TA', '').replace('MR', ''): {\n",
    "        'models': {\n",
    "            'XGB': None\n",
    "        },\n",
    "        'prediction_columns': None\n",
    "    } for juri in juris \n",
    "}\n",
    "\n",
    "for train_name, prefix, suffix in zip(juris, prefixes, suffixes):\n",
    "    juri_name = train_name.replace('TA', '').replace('MR', '')\n",
    "    model_dir = DATA_DIR.parent / 'models' / 'trained' / train_name / f'{train_name.lower()}_{prefix}_{suffix}_dir'\n",
    "    for model_type in ['XGB']:\n",
    "        with open(model_dir / f'train_{model_type}_timehorizon_{train_name.lower()}_{prefix}_{suffix}.pkl', 'rb') as f:\n",
    "            model_dict[juri_name]['models'][model_type] = pickle.load(f)\n",
    "    with open(model_dir  / f'train_labels_columns_{train_name.lower()}_{prefix}_{suffix}.pkl', 'rb') as f:\n",
    "        model_dict[juri_name]['prediction_columns'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {\n",
    "    'WA': '#004259', \n",
    "    'NZ': '#A72326', \n",
    "    'NSW': '#E8A602', \n",
    "    'VIC': '#356130'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions of raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprpcoessing states to turn normalised features back into raw data space\n",
    "#  \n",
    "with open(DATA_DIR.parent / 'models' / 'preprocessing_state' / 'mrwa' / 'preprocessing_state_dict_mrwa_final.sav', 'rb') as f:\n",
    "    mrwa_prepro_dict = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR.parent / 'models' / 'preprocessing_state' / 'nzta' / 'preprocessing_state_dict_nzta_final.sav', 'rb') as f:\n",
    "    nzta_prepro_dict = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR.parent / 'models' / 'preprocessing_state' / 'vic' / 'preprocessing_state_dict.sav', 'rb') as f:\n",
    "    vic_prepro_dict = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR.parent / 'models' / 'preprocessing_state' / 'nsw' / 'preprocessing_state_dict.sav', 'rb') as f:\n",
    "    nsw_prepro_dict = pickle.load(f)\n",
    "    \n",
    "from data import DATA_DIR\n",
    "\n",
    "encoded_train_mrwa = train_flattened_mrwa_truncated.copy()\n",
    "for col, scaler in mrwa_prepro_dict['scaler'].items():\n",
    "    encoded_train_mrwa.loc[:, col] = scaler.inverse_transform(encoded_train_mrwa[col + '_df0|idx=0'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "encoded_train_nzta = train_flattened_nzta_truncated.copy()\n",
    "for col, scaler in nzta_prepro_dict['scaler'].items():\n",
    "    encoded_train_nzta.loc[:, col] = scaler.inverse_transform(encoded_train_nzta[col + '_df0|idx=0'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "encoded_train_nsw = train_flattened_nsw_truncated.copy()\n",
    "for col, scaler in nsw_prepro_dict['scaler'].items():\n",
    "    encoded_train_nsw.loc[:, col] = scaler.inverse_transform(encoded_train_nsw[col].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "encoded_train_vic = train_flattened_vic_truncated.copy()\n",
    "for col, scaler in vic_prepro_dict['scaler'].items():\n",
    "    encoded_train_vic.loc[:, col] = scaler.inverse_transform(encoded_train_vic[col + '|idx=0'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "encoded_train_nsw = encoded_train_nsw.rename(columns={'iri_iwp': 'IRI'})\n",
    "# mrwa and nzta datsets differ in Pavement_Type features, need casting so these datasets have the same features\n",
    "encoded_train_mrwa = encoded_train_mrwa.rename(columns={'Pavement Type_Flexible_df0|idx=0': 'Pavement Type_Flexible', 'Surface Material_SS_df0|idx=0': 'Surface Material_SS'})\n",
    "encoded_train_mrwa.loc[:, 'Pavement Type_Rigid'] = 1 - encoded_train_mrwa['Pavement Type_Flexible'] - encoded_train_mrwa['Pavement Type_Other']\n",
    "encoded_train_mrwa = encoded_train_mrwa[[col for col in encoded_train_mrwa if not col.endswith('idx=0')]]\n",
    "encoded_train_nzta = encoded_train_nzta.rename(columns={'Pavement Type_Flexible_df0|idx=0': 'Pavement Type_Flexible', 'Pavement Type_Rigid_df0|idx=0': 'Pavement Type_Rigid', 'Surface Material_SS_df0|idx=0': 'Surface Material_SS'})\n",
    "encoded_train_nzta = encoded_train_nzta[[col for col in encoded_train_nzta if not col.endswith('idx=0')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(12, 12))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, feature in enumerate(sorted(set(encoded_train_mrwa.columns).intersection(set(encoded_train_nzta.columns).intersection(set(encoded_train_nsw.columns))))):\n",
    "    ax = axs[i]\n",
    "    ax.set_title(feature)\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "    for df, title in [\n",
    "        (encoded_train_mrwa, 'WA'), \n",
    "        (encoded_train_nzta, 'NZ'), \n",
    "        (encoded_train_nsw, 'NSW'), \n",
    "    ]:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "        feature_df = feature_df.append(pd.DataFrame({\n",
    "            feature: df[feature],\n",
    "            'Jurisdiction': [title] * len(df)\n",
    "        }))\n",
    "\n",
    "    \n",
    "    if feature in {'D0', 'D200'}: # exception: normalize this:\n",
    "        feature_df.loc[:, feature] = feature_df.groupby(['Jurisdiction'])[feature].transform(lambda x: (x - np.mean(x)) / np.std(x))\n",
    "        sns.kdeplot(data=feature_df, x=feature, hue='Jurisdiction', ax=ax, palette=palette, legend=True, multiple='layer', shade=True, common_norm=False)\n",
    "        ax.set_title(feature + ' - Normalized')\n",
    "    elif feature_df[feature].nunique() > 2: # otherwise, plot unnormalised data \n",
    "        sns.kdeplot(data=feature_df, x=feature, hue='Jurisdiction', ax=ax, palette=palette, legend=True, multiple='layer', shade=True, common_norm=False)\n",
    "    else:\n",
    "        # binary feature gets plotted using bar plots\n",
    "        feature_df = feature_df.groupby(['Jurisdiction'])[feature].value_counts().rename('Percentage')\n",
    "        feature_df = feature_df / feature_df.sum(axis=0, level=0)\n",
    "        feature_df = feature_df.reset_index()\n",
    "        feature_df.loc[:, feature] = feature_df.replace({feature_df[feature].min(): 'No', feature_df[feature].max(): 'Yes'}) \n",
    "        sns.barplot(data=feature_df, x=feature, y='Percentage', hue='Jurisdiction', ax=ax, palette=palette, alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel(None)\n",
    "\n",
    "fig.suptitle('Distribution of features by jurisdictions')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.savefig(save_fig_dir / 'raw_data_distributions_all_jurisdictions.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all datasets for ease of plotting\n",
    "\n",
    "value_cols = ['Surface age',\n",
    "    'Pavement age',\n",
    "    'AADT',\n",
    "    'HeavyIndex',\n",
    "    'IRI',\n",
    "    'Rutting mm',\n",
    "    'Crack%',\n",
    "    'D0',\n",
    "    'D200'\n",
    "]\n",
    "\n",
    "wa = encoded_train_mrwa[value_cols].copy()\n",
    "nz = encoded_train_nzta[value_cols].copy()\n",
    "nsw = encoded_train_nsw[value_cols].copy()\n",
    "\n",
    "wa['Jurisdiction'] = 'WA'\n",
    "nz['Jurisdiction'] = 'NZ'\n",
    "nsw['Jurisdiction'] = 'NSW'\n",
    "\n",
    "wa_labels = train_flattened_mrwa_labels.copy()\n",
    "nz_labels = train_flattened_nzta_labels.copy()\n",
    "nsw_labels = train_flattened_nsw_labels.copy()\n",
    "wa_labels['Jurisdiction'] = 'WA'\n",
    "nz_labels['Jurisdiction'] = 'NZ'\n",
    "nsw_labels['Jurisdiction'] = 'NSW'\n",
    "\n",
    "labels = pd.concat([\n",
    "    wa_labels, nz_labels, nsw_labels\n",
    "], ignore_index=True)\n",
    "\n",
    "all = pd.concat([\n",
    "    wa, nz, nsw\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column  in {'Resurfacing_SS', 'Resurfacing_AC', 'Rehabilitation'}:\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, feature in enumerate(all.columns):\n",
    "        if feature in ['Jurisdiction', 'embed_x', 'embed_y']:\n",
    "            continue\n",
    "\n",
    "        if feature in ['D0', 'D200']: # normalise these two only\n",
    "            all.loc[:, feature] = all.groupby(['Jurisdiction'])[feature].transform(lambda x: (x - np.mean(x)) / np.std(x))\n",
    "\n",
    "        ax = axs[i]\n",
    "        ax.set_title(feature)\n",
    "        pos_df = all.loc[\n",
    "            labels.loc[:, labels.columns.get_level_values(1) == column].any(axis=1), :\n",
    "        ]\n",
    "\n",
    "        sns.kdeplot(data=pos_df, x=feature, hue='Jurisdiction', ax=ax, shade=1, palette=palette, common_norm=False)\n",
    "\n",
    "        if feature in ['D0', 'D200']:\n",
    "            ax.set_title(feature + ' - Normalised')\n",
    "        else:\n",
    "            ax.set_xlabel('')\n",
    "\n",
    "    fig.suptitle(f'Distribution of raw feature values of sections treated with {column}')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(save_fig_dir / f'flattened_data_distributions_conditioned_{column}.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "treatments = ['Resurfacing_SS', 'Resurfacing_AC', 'Rehabilitation']\n",
    "treatments_2 = ['Major Patching', 'Regulation', 'Retexturing'] # rare class which need different axis to show different scale\n",
    "xticks = np.arange(3)\n",
    "width = 0.1\n",
    "\n",
    "for i, juri, labels in zip(range(4), ['WA', 'NZ', 'NSW', 'VIC'], [train_flattened_mrwa_labels, train_flattened_nzta_labels, train_flattened_nsw_labels, train_flattened_vic_labels]):\n",
    "    # filter out 10 yrs plus\n",
    "    labels : pd.DataFrame = labels[labels.columns[~labels.columns.get_level_values(0).isin({'Treatment between 10 to 30 years'})]]\n",
    "    labels = labels[[col for col in labels.columns if 'Unnamed' not in col[1]]] # drop no project\n",
    "    labels = labels.swaplevel(axis=1).sum(axis=1, level=0) # count treatments by categories\n",
    "    bars1 = ax1.bar(\n",
    "        height=[labels[t].mean() for t in treatments if t in labels.columns], # average number of treatments over 10 year period\n",
    "        x=[pos + i * width for pos, t in enumerate(treatments) if t in labels.columns],\n",
    "        width=width,\n",
    "        label=juri,\n",
    "        color=palette[juri]\n",
    "    )\n",
    "    bars2 = ax2.bar(\n",
    "        height=[labels[t].mean() for t in treatments_2 if t in labels.columns], # average number of treatments over 10 year period\n",
    "        x=[pos + i * width for pos, t in enumerate(treatments_2) if t in labels.columns],\n",
    "        width=width,\n",
    "        label=juri,\n",
    "        color=palette[juri]\n",
    "    )\n",
    "\n",
    "ax1.set_xticks(xticks + 3 / 2 * width)\n",
    "ax1.set_xticklabels(treatments)\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.yaxis.grid(True)\n",
    "ax1.legend()\n",
    "ax2.set_xticks(xticks + 3 / 2 * width)\n",
    "ax2.set_xticklabels(treatments_2)\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.yaxis.set_label_position('right')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.yaxis.grid(True)\n",
    "ax2.legend(loc='upper left')\n",
    "plt.suptitle(\n",
    "    'Average number of treatments planned on one short road section over 10 years\\n'+\\\n",
    "    'Differences in frequency may imply differences in selecting treatments given the same conditions',\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_fig_dir / 'freq_true_labels.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://linuxtut.com/en/92c21048bacadce811ec/\n",
    "def set_hierarchical_xlabels(index, ax=None,\n",
    "                             bar_xmargin=0.1, #Margins on the left and right ends of the line, X-axis scale\n",
    "                             bar_yinterval=0.1, #Relative value with the vertical spacing of the line and the length of the Y axis as 1?\n",
    "                            ):\n",
    "    from itertools import groupby\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    assert isinstance(index, pd.MultiIndex)\n",
    "    labels = ax.set_xticklabels([s for *_, s in index])\n",
    "    for lb in labels:\n",
    "        lb.set_rotation(0)\n",
    "\n",
    "    transform = ax.get_xaxis_transform()\n",
    "\n",
    "    for i in range(1, len(index.codes)):\n",
    "        xpos0 = -0.5 #Coordinates on the left side of the target group\n",
    "        for (*_, code), codes_iter in groupby(zip(*index.codes[:-i])):\n",
    "            xpos1 = xpos0 + sum(1 for _ in codes_iter) #Coordinates on the right side of the target group\n",
    "            ax.text((xpos0+xpos1)/2, (bar_yinterval * (-i-0.1)),\n",
    "                    index.levels[-i-1][code],\n",
    "                    transform=transform,\n",
    "                    ha=\"center\", va=\"top\")\n",
    "            ax.add_line(Line2D([xpos0+bar_xmargin, xpos1-bar_xmargin],\n",
    "                               [bar_yinterval * -i]*2,\n",
    "                               transform=transform,\n",
    "                               color=\"k\", clip_on=False))\n",
    "            xpos0 = xpos1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from data import DATA_DIR\n",
    "import pickle\n",
    "\n",
    "df = []\n",
    "\n",
    "non_zero_freq = pd.DataFrame()\n",
    "# filter out 10 yrs plus\n",
    "for juri, labels in zip(['WA', 'NZ', 'NSW', 'VIC'], [train_flattened_mrwa_labels, train_flattened_nzta_labels, train_flattened_nsw_labels, train_flattened_vic_labels]):\n",
    "    labels = labels[labels.columns[~labels.columns.get_level_values(0).isin({'Treatment between 10 to 30 years', 'no_project_flag'})]]\n",
    "    non_zero_freq[juri] = labels.sum(axis=0) / len(labels) * 100\n",
    "\n",
    "def bar_frequency_label(drop=None, ax=None, color_shift=0):\n",
    "    x = non_zero_freq.copy().reset_index()\n",
    "    x = x.drop(columns=drop)\n",
    "    x.loc[:, 'level_0'] = x['level_0'].str.replace(r'^Treatment (between|within) ', '', regex=True)\n",
    "    x= x\\\n",
    "        .sort_values(by='level_0', key=lambda series: series.str.replace(r'^(\\d+)( to )?(\\d+)? years?', r'\\1\\3', regex=True).astype(int))\\\n",
    "        .sort_values(by='level_1', kind='stable')\\\n",
    "        .set_index(['level_1', 'level_0'])\n",
    "    x.index.names = (None, None)\n",
    "\n",
    "    ax = ax or plt.subplots(figsize=(28, 12))[1]\n",
    "    fig = ax.get_figure()\n",
    "    x.plot.bar(\n",
    "        color=palette,\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )\n",
    "    set_hierarchical_xlabels(x.index)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "    for bars in ax.containers:\n",
    "        ax.bar_label(bars, padding=3, fmt='%.1f')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_title('% of Positive Label')\n",
    "    ax.legend(bbox_to_anchor=(1, 0), loc=\"lower left\", title='Train / Eval datasets')\n",
    "    ax.grid(True)\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    return ax\n",
    "\n",
    "ax = bar_frequency_label(drop=[])\n",
    "ax.set_title('% of Positive Labels')\n",
    "ax.get_figure().savefig(save_fig_dir / 'distribution_of_predictions.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
