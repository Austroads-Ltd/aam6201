{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import src.util as util\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import DATA_DIR\n",
    "SUFFIX = 'even_split'\n",
    "\n",
    "REPORT_DIR = DATA_DIR.parent / 'reports' / 'figures' / 'cluster'\n",
    "if REPORT_DIR.exists() is False:\n",
    "    REPORT_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from src import DATA_DIR\n",
    "\n",
    "\n",
    "# Load MRWA Data\n",
    "DATASET_NAME = 'MRWA'\n",
    "if SUFFIX == 'even_split':\n",
    "    experiment_suffix = 'mrwa_final_even_split'\n",
    "elif SUFFIX == 'no_bootstrap':\n",
    "    experiment_suffix = 'mrwa_final_no_offset'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "wa_valid_feature = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') /  f'valid_flattened_data{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv') \n",
    "wa_train_feature = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_data{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv') \n",
    "wa_valid_labels = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') /  f'valid_flattened_labels{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv', header=[0, 1]) \n",
    "wa_train_labels = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') /  f'train_flattened_labels{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv', header=[0, 1]) \n",
    "wa_valid_index = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'valid_flattened_index{\"_\" + experiment_suffix.replace(\"_no_offset\", \"\") if experiment_suffix else \"\"}.csv') \n",
    "wa_train_index = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_index{\"_\" + experiment_suffix.replace(\"_no_offset\", \"\") if experiment_suffix else \"\"}.csv') \n",
    "experiment_suffix = 'mrwa_final_' + SUFFIX\n",
    "\n",
    "# original projects\n",
    "wa_projects = util.load_data(source=DATA_DIR / 'interim' / DATASET_NAME / 'cleaned_projects.csv')\n",
    "\n",
    "model_dir = REPORT_DIR.parent.parent.parent / 'models' / 'trained' / DATASET_NAME / (experiment_suffix + '_dir') \n",
    "# Load Prediction columns\n",
    "with open(model_dir / f'train_labels_columns_{experiment_suffix}.pkl', 'rb') as f:\n",
    "    wa_prediction_cols = pickle.load(f)\n",
    "# Load model\n",
    "with open(model_dir / f'train_XGB_timehorizon_{experiment_suffix}.pkl', 'rb') as f:\n",
    "    wa_models = pickle.load(f)\n",
    "\n",
    "# join all\n",
    "wa_features = wa_valid_feature \n",
    "wa_index = wa_valid_index \n",
    "\n",
    "# cast year offset\n",
    "year_offset = ((wa_projects['Date Treatment'].astype(np.datetime64) - wa_projects['Date Planned'].astype(np.datetime64)) / np.timedelta64(1, 'Y'))\n",
    "for col in ['Treatment within 1 year', 'Treatment between 1 to 3 years', 'Treatment between 3 to 5 years', 'Treatment between 5 to 10 years', 'Treatment between 10 to 30 years']:\n",
    "    wa_projects.loc[:, col] = False\n",
    "wa_projects.loc[year_offset < 1, 'Treatment within 1 year'] = True\n",
    "wa_projects.loc[(year_offset > 1) & (year_offset <= 3), 'Treatment between 1 to 3 years'] = True\n",
    "wa_projects.loc[((year_offset > 3) & (year_offset <= 5)), 'Treatment between 3 to 5 years'] = True\n",
    "wa_projects.loc[((year_offset > 5) & (year_offset <= 10)), 'Treatment between 5 to 10 years'] = True\n",
    "wa_projects.loc[((year_offset > 10) & (year_offset <= 30)), 'Treatment between 10 to 30 years'] = True\n",
    "\n",
    "# cast to int to avoid floating point erros (e.g. 400 + e-32 > 400 + e-33)\n",
    "wa_index.loc[:, ['Start', 'End']] = wa_index[['Start', 'End']].astype(int)\n",
    "wa_projects.loc[:, ['Start', 'End']] = wa_projects[['Start', 'End']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NZ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from src import DATA_DIR\n",
    "\n",
    "# Load MRWA Data\n",
    "DATASET_NAME = 'NZTA'\n",
    "if SUFFIX == 'even_split':\n",
    "    experiment_suffix = 'nzta_final_even_split'\n",
    "elif SUFFIX == 'no_bootstrap':\n",
    "    experiment_suffix = 'nzta_final_no_offset'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "nz_valid_feature = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') /  f'valid_flattened_data{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv') \n",
    "nz_train_feature = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_data{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv') \n",
    "nz_valid_labels = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'valid_flattened_labels{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv', header=[0, 1]) \n",
    "nz_train_labels = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_labels{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv', header=[0, 1]) \n",
    "nz_valid_index = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'valid_flattened_index{\"_\" + experiment_suffix.replace(\"_no_offset\", \"\") if experiment_suffix else \"\"}.csv') \n",
    "nz_train_index = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_index{\"_\" + experiment_suffix.replace(\"_no_offset\", \"\") if experiment_suffix else \"\"}.csv') \n",
    "experiment_suffix = 'nzta_final_' + SUFFIX\n",
    "\n",
    "# original projects\n",
    "nz_projects = util.load_data(source=DATA_DIR / 'interim' / DATASET_NAME / 'cleaned_projects.csv')\n",
    "nz_projects = nz_projects.rename(columns={'Treatment Category': 'Treatment'})\n",
    "\n",
    "model_dir = REPORT_DIR.parent.parent.parent / 'models' / 'trained' / DATASET_NAME / (experiment_suffix + '_dir') \n",
    "# Load Prediction columns\n",
    "with open(model_dir / f'train_labels_columns_{experiment_suffix}.pkl', 'rb') as f:\n",
    "    nz_prediction_cols = pickle.load(f)\n",
    "# Load model\n",
    "with open(model_dir / f'train_XGB_timehorizon_{experiment_suffix}.pkl', 'rb') as f:\n",
    "    nz_models = pickle.load(f)\n",
    "\n",
    "# join all\n",
    "nz_features = nz_valid_feature \n",
    "nz_index = nz_valid_index\n",
    "\n",
    "# cast year offset\n",
    "year_offset = ((nz_projects['Date Treatment'].astype(np.datetime64) - nz_projects['Date Planned'].astype(np.datetime64)) / np.timedelta64(1, 'Y'))\n",
    "for col in ['Treatment within 1 year', 'Treatment between 1 to 3 years', 'Treatment between 3 to 5 years', 'Treatment between 5 to 10 years', 'Treatment between 10 to 30 years']:\n",
    "    nz_projects.loc[:, col] = False\n",
    "nz_projects.loc[year_offset < 1, 'Treatment within 1 year'] = True\n",
    "nz_projects.loc[(year_offset > 1) & (year_offset <= 3), 'Treatment between 1 to 3 years'] = True\n",
    "nz_projects.loc[((year_offset > 3) & (year_offset <= 5)), 'Treatment between 3 to 5 years'] = True\n",
    "nz_projects.loc[((year_offset > 5) & (year_offset <= 10)), 'Treatment between 5 to 10 years'] = True\n",
    "nz_projects.loc[((year_offset > 10) & (year_offset <= 30)), 'Treatment between 10 to 30 years'] = True\n",
    "\n",
    "# cast to int to avoid floating point erros (e.g. 400 + e-32 > 400 + e-33)\n",
    "nz_index.loc[:, ['Start', 'End']] = nz_index[['Start', 'End']].astype(int)\n",
    "nz_projects.loc[:, ['Start', 'End']] = nz_projects[['Start', 'End']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD VIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from src import DATA_DIR\n",
    "\n",
    "# Load MRWA Data\n",
    "DATASET_NAME = 'VIC'\n",
    "if SUFFIX == 'even_split':\n",
    "    experiment_suffix = 'vic_final_even_split'\n",
    "elif SUFFIX == 'no_bootstrap':\n",
    "    experiment_suffix = 'vic_final_no_offset'\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "vic_valid_feature = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') /  f'valid_flattened_data{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv') \n",
    "vic_train_feature = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_data{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv') \n",
    "vic_valid_labels = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'valid_flattened_labels{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv', header=[0, 1]) \n",
    "vic_train_labels = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_labels{\"_\" + experiment_suffix if experiment_suffix else \"\"}.csv', header=[0, 1]) \n",
    "vic_valid_index = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'valid_flattened_index{\"_\" + experiment_suffix.replace(\"_no_offset\", \"\") if experiment_suffix else \"\"}.csv') \n",
    "vic_train_index = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '') / f'train_flattened_index{\"_\" + experiment_suffix.replace(\"_no_offset\", \"\") if experiment_suffix else \"\"}.csv') \n",
    "experiment_suffix = 'vic_final_' + SUFFIX\n",
    "\n",
    "# original projects\n",
    "projects = util.load_data(DATA_DIR / 'raw' / 'VIC' / 'AAM6201 Data Reques' / 'Work Program' / 'Pavement Diary since 2014_2019.xlsx', sheet_name=0)\n",
    "treatment_lookup = util.load_data(DATA_DIR.parent / \"references\" / \"TreatmentCategory.csv\")\n",
    "treatment_lookup = treatment_lookup[treatment_lookup['Jurisdiction'] == 'VIC']\n",
    "projects = projects.rename(columns={\"Route Number\": \"Road_Number\",\n",
    "                                    \"From Measure\": \"From_Measure\"})\n",
    "cleaned_projects = projects.dropna(\n",
    "    subset=['Road_Number', 'Direction', 'From_Measure', 'Length', 'Treatment Date', 'Treatment Type']\n",
    ").copy()\n",
    "cleaned_projects.loc[cleaned_projects['Direction'].str.contains('Forward'), 'Direction'] = 'Forward'\n",
    "cleaned_projects.loc[cleaned_projects['Direction'].str.contains('Reverse'), 'Direction'] = 'Reverse'\n",
    "cleaned_projects = cleaned_projects[cleaned_projects['Direction'].isin({'Forward', 'Reverse'})]\n",
    "cleaned_projects[\"Treatment Category\"] = cleaned_projects[\"Treatment Type\"]\n",
    "cleaned_projects[\"Treatment Category\"] = cleaned_projects[\"Treatment Category\"].replace(dict(zip(treatment_lookup[\"Specific Category Value\"], treatment_lookup[\"Generic Category\"])))\n",
    "cleaned_projects = cleaned_projects.drop(index=cleaned_projects[~cleaned_projects[\"Treatment Category\"].isin(treatment_lookup[\"Generic Category\"])].index)\n",
    "cleaned_projects = cleaned_projects[[\"Road_Number\", \"Route Name\", \"Direction\", \"From_Measure\", \"To Measure\", \"Length\", \"Treatment Date\", \"Treatment Category\"]]\n",
    "cleaned_projects = cleaned_projects.rename(columns={\"Road_Number\": \"RoadID\",\n",
    "                                                    \"From_Measure\": \"Start\",\n",
    "                                                    \"Treatment Date\": \"Date Treatment\"})\n",
    "cleaned_projects = cleaned_projects.drop_duplicates()\n",
    "cleaned_projects = cleaned_projects[cleaned_projects['Treatment Category'].notna()]\n",
    "cleaned_projects.loc[:, 'Date Treatment'] = cleaned_projects['Date Treatment'].astype(np.datetime64)\n",
    "vic_projects = cleaned_projects\n",
    "vic_projects = vic_projects.rename(columns={'Treatment Category': 'Treatment'})\n",
    "\n",
    "\n",
    "model_dir = REPORT_DIR.parent.parent.parent / 'models' / 'trained' / DATASET_NAME / (experiment_suffix + '_dir') \n",
    "# Load Prediction columns\n",
    "with open(model_dir / f'train_labels_columns_{experiment_suffix}.pkl', 'rb') as f:\n",
    "    vic_prediction_cols = pickle.load(f)\n",
    "# Load model\n",
    "with open(model_dir / f'train_XGB_timehorizon_{experiment_suffix}.pkl', 'rb') as f:\n",
    "    vic_models = pickle.load(f)\n",
    "\n",
    "# join all\n",
    "vic_features = vic_valid_feature \n",
    "vic_index = vic_valid_index\n",
    "\n",
    "# cast year offset\n",
    "vic_all_index = pd.concat([vic_index, vic_valid_index], axis=0, ignore_index=True)\n",
    "vic_all_offsets = pd.concat([vic_train_feature['offset_month|idx=0'], vic_valid_feature['offset_month|idx=0']], ignore_index=True, axis=0)\n",
    "year_offset = vic_all_offsets / 12\n",
    "vic_all_index['year_offset'] = year_offset\n",
    "vic_all_index = vic_all_index[vic_all_index['num_treatments'] > 0]\n",
    "year_offset = vic_all_index['year_offset']\n",
    "\n",
    "def parse_list_of_index_series(series):\n",
    "    return [int(elem) for x in series for elem in x.split(',')]\n",
    "\n",
    "for col in ['Treatment within 1 year', 'Treatment between 1 to 3 years', 'Treatment between 3 to 5 years', 'Treatment between 5 to 10 years', 'Treatment between 10 to 30 years']:\n",
    "    vic_projects.loc[:, col] = False\n",
    "\n",
    "vic_projects.loc[\n",
    "    parse_list_of_index_series(vic_all_index[year_offset < 1]['treatment_idx']), \n",
    "    'Treatment within 1 year'\n",
    "] = True \n",
    "vic_projects.loc[\n",
    "    parse_list_of_index_series(vic_all_index[(year_offset > 1) & (year_offset <= 3)]['treatment_idx']), \n",
    "    'Treatment between 1 to 3 years'\n",
    "] = True \n",
    "vic_projects.loc[\n",
    "    parse_list_of_index_series(vic_all_index[(year_offset > 3) & (year_offset <= 5)]['treatment_idx']), \n",
    "    'Treatment between 3 to 5 years'\n",
    "] = True\n",
    "vic_projects.loc[\n",
    "    parse_list_of_index_series(vic_all_index[(year_offset > 5) & (year_offset <= 10)]['treatment_idx']), \n",
    "    'Treatment between 5 to 10 years'\n",
    "] = True \n",
    "vic_projects.loc[\n",
    "    parse_list_of_index_series(vic_all_index[(year_offset > 10) & (year_offset <= 30)]['treatment_idx']), \n",
    "    'Treatment between 10 to 30 years'\n",
    "] = True\n",
    "\n",
    "# cast to int to avoid floating point erros (e.g. 400 + e-32 > 400 + e-33)\n",
    "vic_projects['End'] = vic_projects['Start'] + vic_projects['Length']\n",
    "vic_index['End'] = vic_index['Start'] + vic_index['Length']\n",
    "vic_train_index['End'] = vic_train_index['Start'] + vic_train_index['Length']\n",
    "vic_index.loc[:, ['Start', 'End']] = vic_index[['Start', 'End']].astype(int)\n",
    "vic_projects.loc[:, ['Start', 'End']] = vic_projects[['Start', 'End']].astype(int)\n",
    "\n",
    "# cast roadid to str\n",
    "vic_projects.loc[:, 'RoadID'] = vic_projects['RoadID'].astype(str)\n",
    "vic_index.loc[:, 'RoadID'] = vic_index['RoadID'].astype(str)\n",
    "vic_train_index.loc[:, 'RoadID'] = vic_train_index['RoadID'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put into dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    'WA': {\n",
    "        'prediction_cols': wa_prediction_cols,\n",
    "        'features': wa_features,\n",
    "        'models': wa_models,\n",
    "        'index': wa_index,\n",
    "        'h_index': ['RoadID', 'Direction'],\n",
    "        'projects': wa_projects\n",
    "    },\n",
    "    'NZ': {\n",
    "        'prediction_cols': nz_prediction_cols,\n",
    "        'features': nz_features,\n",
    "        'models': nz_models,\n",
    "        'index': nz_index,\n",
    "        'h_index': ['RoadID'],\n",
    "        'projects': nz_projects\n",
    "    },\n",
    "    'VIC': {\n",
    "        'prediction_cols': vic_prediction_cols,\n",
    "        'features': vic_features,\n",
    "        'models': vic_models,\n",
    "        'index': vic_index,\n",
    "        'h_index': ['RoadID', 'Direction'],\n",
    "        'projects': vic_projects\n",
    "    }\n",
    "}\n",
    "\n",
    "year_map_dict = {\n",
    "    'Treatment within 1 year': 'Year 1',\n",
    "    'Treatment between 1 to 3 years': 'Year 2 - 3',\n",
    "    'Treatment between 3 to 5 years': 'Year 4 - 5',\n",
    "    'Treatment between 5 to 10 years': 'Year 6 - 10',\n",
    "}\n",
    "\n",
    "year_order_dict = {\n",
    "    'Treatment within 1 year': 0,\n",
    "    'Treatment between 1 to 3 years': 1, \n",
    "    'Treatment between 3 to 5 years': 2, \n",
    "    'Treatment between 5 to 10 years': 3,\n",
    "}\n",
    "\n",
    "treatment_type_order = {\n",
    "    'Resurfacing_SS': 0,\n",
    "    'Resurfacing_AC': 1,\n",
    "    'Major Patching': 2,\n",
    "    'Rehabilitation': 3,\n",
    "    'Retexturing': 4,\n",
    "    'Regulation': 5\n",
    "}\n",
    "\n",
    "treatment_type_colors = {\n",
    "    'Resurfacing_SS': 'tab:purple',\n",
    "    'Resurfacing_AC': 'tab:brown',\n",
    "    'Major Patching': 'tab:gray',\n",
    "    'Rehabilitation': 'tab:olive',\n",
    "    'Retexturing': 'tab:cyan',\n",
    "    'Regulation': 'tab:pink',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTIL\n",
    "\n",
    "Define method for:\n",
    "* visualising clustered results vs true projects\n",
    "* quantify metrics for the sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input two dataframes on same hierarchical index\n",
    "# cast them both to fixed sections\n",
    "# groupby fixed sections and count, \n",
    "from typing import List, Tuple\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mpl_colors\n",
    "\n",
    "def cast_to_fixed_section(df: pd.DataFrame, fixed_length: float=100):\n",
    "    split_df = df.copy()\n",
    "    split_df = split_df.reset_index(drop=True)\n",
    "\n",
    "    split_df['Start_Index'] = (split_df['Start'] // fixed_length)\n",
    "    split_df['End_Index'] = (split_df['End']) // fixed_length - (split_df['End'] % fixed_length == 0)\n",
    "    split_df.loc[:, 'Duplicate_Count'] = (split_df['End_Index'] - split_df['Start_Index'] + 1)\n",
    "    split_df = split_df.loc[split_df.index.repeat(split_df['Duplicate_Count'])]\n",
    "    split_df = split_df.reset_index().rename(columns={'index': 'position'})\n",
    "\n",
    "    # assign column matching position with the earliest index with that position\n",
    "    index_position_lookup = split_df.drop_duplicates(subset=['position'], keep='first')['position'].reset_index()\n",
    "    split_df = split_df.set_index('position')\n",
    "    split_df.loc[:, 'original_position'] = index_position_lookup.set_index('position')['index']\n",
    "    split_df = split_df.reset_index()\n",
    "    split_df.loc[:, 'fixed_length_Index'] = split_df['Start_Index'] + split_df.index - split_df['original_position']\n",
    "    split_df = split_df.drop(columns=['position', 'original_position', 'Start_Index', 'End_Index', 'Duplicate_Count'])\n",
    "\n",
    "    # calculate length contribution\n",
    "    split_df['fixed_length_Start'] = split_df['fixed_length_Index'] * fixed_length\n",
    "    split_df['fixed_length_End'] = split_df['fixed_length_Start'] + fixed_length\n",
    "    split_df['Length_Contribution'] = split_df[['End', 'fixed_length_End']].min(axis=1) - split_df[['Start', 'fixed_length_Start']].max(axis=1)\n",
    "\n",
    "    # make sure newly minted start ends are used as index instead of the old ones\n",
    "    split_df.drop(columns=['fixed_length_Index', 'Start', 'End'], inplace=True)\n",
    "    split_df.rename(columns={'fixed_length_Start': 'Start', 'fixed_length_End': 'End'}, inplace=True)\n",
    "\n",
    "    return split_df\n",
    "    \n",
    "def merge_intervals(lst: List[Tuple[float, float]]):\n",
    "    \"\"\"[(a, b), (b-1, c)] -> [(a, c)]\"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return lst\n",
    "        \n",
    "    lst = sorted(lst, key=lambda x: x[0]) # sort by start\n",
    "    curr_start, curr_end = lst[0]\n",
    "    new_lst = []\n",
    "    for lst_pt in range(1, len(lst)):\n",
    "        # check valid extension \n",
    "        if (lst[lst_pt][0] <= curr_end) and (lst[lst_pt][1] > curr_end):\n",
    "            curr_end = lst[lst_pt][1]\n",
    "            lst_pt += 1\n",
    "        # beyond range \n",
    "        elif lst[lst_pt][0] > curr_end:\n",
    "            new_lst.append((curr_start, curr_end))\n",
    "            curr_start, curr_end = lst[lst_pt]\n",
    "    # append current interval\n",
    "    new_lst.append((curr_start, curr_end))\n",
    "    return new_lst\n",
    "\n",
    "assert merge_intervals([(1, 1), (2, 2)]) == [(1, 1), (2, 2)]\n",
    "assert merge_intervals([(1, 2), (2, 3), (3, 4), (5, 7), (6, 8)]) == [(1, 4), (5, 8)]\n",
    "assert merge_intervals([]) == []\n",
    "assert merge_intervals([(1, 1), (1, 2), (2, 3), (3, 4)]) == [(1, 4)]\n",
    "\n",
    "\n",
    "def extend_index(index_df: pd.DataFrame, h_index: tuple) -> pd.DataFrame:\n",
    "    \"\"\"Extend missing start-end sections with rows of nan\"\"\"\n",
    "    final_df_lst = []\n",
    "    for index_tup, inner_index in index_df.groupby(h_index):\n",
    "        intervals = list(inner_index[['Start', 'End']].itertuples(index=False, name=None))\n",
    "        intervals = merge_intervals(intervals)\n",
    "        missing_intervals = [(0, intervals[0][0])]\n",
    "        missing_intervals.extend([\n",
    "            (prev[1], nxt[0]) for prev, nxt in zip(intervals, intervals[1:])\n",
    "        ])\n",
    "\n",
    "        for interval in missing_intervals:\n",
    "            inner_index = inner_index.append({\n",
    "                'Start': interval[0], 'End': interval[1]\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "        for key, val in zip(h_index, index_tup):\n",
    "            inner_index.loc[:, key] = val\n",
    "\n",
    "        final_df_lst.append(inner_index)\n",
    "    return pd.concat(final_df_lst, axis=0, ignore_index=True)\n",
    "\n",
    "def check_inputted_frames(actual_df: pd.DataFrame, predicted_df: pd.DataFrame, h_index: tuple):\n",
    "    assert actual_df.drop_duplicates(subset=h_index).shape[0] <= 1 # can be empty\n",
    "    assert predicted_df.drop_duplicates(subset=h_index).shape[0] <= 1 # can be empty\n",
    "    # assert actual_df.drop_duplicates(subset=['Treatment', 'Treatment Time']).shape[0] <= 1\n",
    "    # assert predicted_df.drop_duplicates(subset=['Treatment', 'Treatment Time']).shape[0] <= 1\n",
    "    if len(predicted_df) > 0 and len(actual_df) > 0:\n",
    "        assert all(predicted_df[h_index].iloc[0].values == actual_df[h_index].iloc[0].values)\n",
    "    #     assert all(predicted_df[['Treatment', 'Treatment Time']].iloc[0].values == actual_df[['Treatment', 'Treatment Time']].iloc[0].values)\n",
    "\n",
    "def plot_compare_projects(\n",
    "    actual_df: pd.DataFrame, \n",
    "    predicted_df: pd.DataFrame, \n",
    "    unclustered_df: pd.DataFrame=None,\n",
    "    bin_width: float=20, # bin width should be smaller than condition section length\n",
    "    h_index: List[str]=['RoadID', 'Direction'],\n",
    "    index_mask: pd.DataFrame=None,\n",
    "    axs: Tuple[plt.Axes]=None,\n",
    "):\n",
    "    \"\"\"Plot number of projects for fixed size bin. Both datasets should contain projects for only one treatment-time pair\"\"\"\n",
    "    if len(actual_df) == 0 and len(predicted_df) == 0:\n",
    "        print(\"No project provided\")\n",
    "        return None, None\n",
    "\n",
    "    if len(actual_df) > 0 and len(predicted_df) > 0:\n",
    "        check_inputted_frames(actual_df, predicted_df, h_index)\n",
    "\n",
    "    # get range for plot\n",
    "    start = np.nanmax(np.nanmin([actual_df['Start'].min(), predicted_df['Start'].min()])- 100, 0)\n",
    "    end = np.nanmax([actual_df['End'].max(), predicted_df['End'].max()]) + 100\n",
    "\n",
    "    # round to nearest bin_width * n\n",
    "    start = (start // bin_width) * bin_width\n",
    "    end = (end // bin_width) * bin_width + (bin_width if end % bin_width > 0 else 0)\n",
    "    assert end > start\n",
    "\n",
    "    all_splits = []\n",
    "\n",
    "    split_actual = cast_to_fixed_section(actual_df[h_index + ['Start', 'End']], fixed_length=bin_width)\n",
    "    split_predicted = cast_to_fixed_section(predicted_df[h_index + ['Start', 'End']], fixed_length=bin_width)\n",
    "\n",
    "    if len(split_actual) > 0:\n",
    "        split_actual['flag'] = 'Ground truth'\n",
    "    if len(split_predicted) > 0:\n",
    "        split_predicted['flag'] = 'After clustering'\n",
    "    all_splits.extend([split_actual, split_predicted])\n",
    "\n",
    "    if unclustered_df is not None:\n",
    "        split_raw = cast_to_fixed_section(unclustered_df[h_index + ['Start', 'End']], fixed_length=bin_width)\n",
    "        split_raw['flag'] = 'Before clustering'\n",
    "        all_splits.append(split_raw)\n",
    "\n",
    "    stacked_split = pd.concat(all_splits, axis=0).reset_index(drop=True)\n",
    "    stacked_count = stacked_split.groupby(['flag', 'Start']).count().iloc[:, 0].rename('Count').reset_index()\n",
    "\n",
    "    colors = mpl_colors.ListedColormap(['#004259', '#002D3D', '#000F14'], name='austroads_cmap') # color correspond to flag index\n",
    "\n",
    "    # make plot\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(len(all_splits), 1, figsize=(15, 7.5))\n",
    "\n",
    "    patches = []\n",
    "    labels = []\n",
    "    for i, flag in enumerate(['Ground truth', 'After clustering'] + (['Before clustering'] if unclustered_df is not None else [])):\n",
    "        if flag not in set(stacked_count['flag']):\n",
    "            continue\n",
    "        flag_df = stacked_count[stacked_count['flag'] == flag].set_index('Start')['Count']\n",
    "        bars = axs[i].bar(\n",
    "            x=flag_df.index, \n",
    "            height=flag_df.values,\n",
    "            width=bin_width,\n",
    "            align='edge',\n",
    "            color=colors(i)\n",
    "        )\n",
    "        patches.append(Patch(facecolor=bars[0].get_facecolor()))\n",
    "        if flag == 'Ground truth':\n",
    "            labels.append('Ground truth projects')\n",
    "        elif flag == 'After clustering':\n",
    "            labels.append('Projects as determined after the clustering step')\n",
    "        elif flag == 'Before clustering':\n",
    "            labels.append('Sections predicted to have treatment out of sections the model see')\n",
    "\n",
    "    # if there is mask, plot mask ranges\n",
    "    if index_mask is not None and len(index_mask) > 1:\n",
    "        assert index_mask[h_index].drop_duplicates().shape[0] == 1\n",
    "        assert index_mask[h_index].iloc[0].to_list() == actual_df[h_index].iloc[0].to_list()\n",
    "\n",
    "        mask_intervals = list(index_mask.sort_values(by=['Start'])[['Start', 'End']].itertuples(index=False, name=None))\n",
    "        mask_intervals = merge_intervals(mask_intervals)\n",
    "\n",
    "        any_mask = False\n",
    "        for mask_start, mask_end in mask_intervals:\n",
    "            axs[-1].fill_betweenx(y=[0, 1], x1=mask_start, x2=mask_end, color='silver')\n",
    "            any_mask = True\n",
    "\n",
    "        if any_mask:\n",
    "            patches.append(Patch(facecolor='silver'))\n",
    "            labels.append('Sections hidden from model')\n",
    "\n",
    "    # set yaxis to discrete integers\n",
    "    for ax in axs:\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_ylim((0, 1))\n",
    "        ax.set_xlim((start, end))\n",
    "        ax.set_xlabel('Chainage (m)')\n",
    "        ax.set_xticks(np.linspace(start, end, num=20))\n",
    "\n",
    "    # axs[0].legend(\n",
    "    #     handles=patches, labels=labels, bbox_to_anchor=(0, 1), loc='lower left'\n",
    "    # )\n",
    "    axs[0].set_title('Ground truth projects', loc='left')\n",
    "    axs[1].set_title('Predicted projects using clustering', loc='left')\n",
    "    axs[2].set_title('Sections predicted to have treatment out of sections the model see', loc='left')\n",
    "    axs[2].legend(\n",
    "        handles=[Patch(facecolor='black', alpha=0.2)],\n",
    "        labels=['Sections hidden from model'],\n",
    "        bbox_to_anchor=(1, 1),\n",
    "        loc='lower right'\n",
    "    )\n",
    "    \n",
    "    return axs, stacked_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for finding invalid labels and ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "def clip_mask(start: float, end: float, df: pd.DataFrame):\n",
    "    # clip end\n",
    "    df.loc[\n",
    "        (df['Start'] < start) &\\\n",
    "        (df['End'] > start) &\\\n",
    "        (df['End'] <= end),\n",
    "        'End'\n",
    "    ] = start\n",
    "    # clip start\n",
    "    df.loc[\n",
    "        (df['Start'] < end) &\\\n",
    "        (df['Start'] >= start) &\\\n",
    "        (df['End'] > end),\n",
    "        'Start'\n",
    "    ] = end\n",
    "    # clip start & end\n",
    "    x = df.loc[\n",
    "        (df['Start'] < start) &\\\n",
    "        (df['End'] > end)\n",
    "    ] \n",
    "    if len(x) > 0:\n",
    "        df.drop(index=x.index, inplace=True)\n",
    "        new_x = x.copy()\n",
    "        x.loc[:, 'End'] = start\n",
    "        new_x.loc[:, 'Start'] = end\n",
    "        df = df.append([new_x, x], ignore_index=True)\n",
    "\n",
    "    # throw away inside \n",
    "    df = df[~(\n",
    "        (df['Start'] >= start) & \\\n",
    "        (df['End'] <= end)\n",
    "    )]\n",
    "    return df\n",
    "\n",
    "def find_mask_ranges(projects: pd.DataFrame, index_df: pd.DataFrame, h_index: Tuple=['RoadID', 'Direction']):\n",
    "    \"\"\"Generate all intervals where there are no spatial index from the condition data. \n",
    "    Removing this set helps improving the results of all algorithms, as these have no information and cannot be predicted\n",
    "\n",
    "    Important: mask only indexes not available in both train AND valid set, as index in train set and valid set are evenly distributed and thus we expect clustering to do well\n",
    "    \"\"\"\n",
    "\n",
    "    disjoint = pd.DataFrame()\n",
    "\n",
    "    for index in index_df[h_index].drop_duplicates().itertuples(index=False, name=None):\n",
    "        index = [str(elem) for elem in index]\n",
    "        filtered_index = index_df[(index_df[h_index].values == index).all(axis=1)]\n",
    "        filtered_projects = projects[(projects[h_index].values == index).all(axis=1)]\n",
    "        sorted_index = filtered_index.sort_values(['Start'])\n",
    "\n",
    "        if len(filtered_projects) == 0:\n",
    "            continue\n",
    "\n",
    "        merged = merge_intervals(list(sorted_index.sort_values(['Start'])[['Start', 'End']].itertuples(index=False, name=None))) # linear scan and condense intervals of all index\n",
    "\n",
    "        # check continuity\n",
    "        if len(merged) > 1: # more than 2 intervals\n",
    "            # retrieve ranges where labels are discontinous and therefore no algorithm can do better\n",
    "            for j in range(1, len(merged)):\n",
    "                prev, curr = merged[j - 1], merged[j]\n",
    "                assert curr[0] > prev[1]\n",
    "                disjoint = pd.concat(\n",
    "                    [disjoint,\n",
    "                    pd.DataFrame({\n",
    "                        'Start': prev[1],\n",
    "                        'End': curr[0],\n",
    "                        **{\n",
    "                            col: [val] for col, val in zip(h_index, index)\n",
    "                        }\n",
    "                    })],\n",
    "                    axis=0\n",
    "                )\n",
    "\n",
    "        # outer disjoint (iff projects overshoots index at either end. If not the case, pruned at the end)\n",
    "        disjoint = pd.concat([\n",
    "            disjoint,\n",
    "            pd.DataFrame({\n",
    "                'Start': [filtered_projects['Start'].min(), filtered_index['End'].max()],\n",
    "                'End': [filtered_index['Start'].min(), filtered_projects['End'].max()],\n",
    "                **{\n",
    "                    col: [val] * 2 for col, val in zip(h_index, index)\n",
    "                }\n",
    "            })\n",
    "        ], axis=0)\n",
    "\n",
    "    disjoint = disjoint.reset_index(drop=True)\n",
    "    disjoint = disjoint[disjoint['Start'] < disjoint['End']].reset_index(drop=True)\n",
    "    return disjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for making quantitative comparison dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compare_projects(\n",
    "    actual_df: pd.DataFrame, \n",
    "    predicted_df: pd.DataFrame, \n",
    "    h_index: List[str]=['RoadID', 'Direction'],\n",
    "):\n",
    "    check_inputted_frames(actual_df, predicted_df, h_index)\n",
    "    actual_intervals = actual_df[actual_df['End'] > actual_df['Start']][['Start', 'End']].itertuples(index=False, name=None)\n",
    "    predicted_intervals = predicted_df[predicted_df['End'] > predicted_df['Start']][['Start', 'End']].itertuples(index=False, name=None)\n",
    "\n",
    "    # merge intervals \n",
    "    actual_intervals = merge_intervals(list(actual_intervals)) \n",
    "    predicted_intervals = merge_intervals(list(predicted_intervals))\n",
    "\n",
    "    # maps which intervals has intersections\n",
    "    actual_intersection : dict = {}\n",
    "    predicted_intersection : dict = {}\n",
    "\n",
    "    # find all intersections (with non overlap assumption. If overlap happens, goodness is over counted, but this should be extremely rare)\n",
    "    actual_pt, predicted_pt = 0, 0\n",
    "    while actual_pt < len(actual_intervals) and predicted_pt < len(predicted_intervals):\n",
    "        actual_start, actual_end = actual_intervals[actual_pt]\n",
    "        predicted_start, predicted_end = predicted_intervals[predicted_pt]\n",
    "\n",
    "        if predicted_start < actual_end and predicted_end > actual_start: # intersect yes\n",
    "            intersection_start, intersection_end = max(actual_start, predicted_start), min(actual_end, predicted_end)\n",
    "            if (actual_start, actual_end) not in actual_intersection:\n",
    "                actual_intersection[(actual_start, actual_end)] = [(intersection_start, intersection_end)]\n",
    "            else:\n",
    "                actual_intersection[(actual_start, actual_end)].append((intersection_start, intersection_end))\n",
    "            if (predicted_start, predicted_end) not in predicted_intersection:\n",
    "                predicted_intersection[(predicted_start, predicted_end)] = [(intersection_start, intersection_end)]\n",
    "            else:\n",
    "                predicted_intersection[(predicted_start, predicted_end)].append((intersection_start, intersection_end))\n",
    "\n",
    "        if predicted_end <= actual_end:\n",
    "            predicted_pt += 1\n",
    "        else:\n",
    "            actual_pt += 1\n",
    "\n",
    "    # compute metrics\n",
    "    result_dict = dict(\n",
    "        metr_true_area_total = sum(y - x for x, y in actual_intervals),\n",
    "        metr_matched_total = sum(y - x for intervals in actual_intersection.values() for x, y in intervals),\n",
    "        metr_predicted_total = sum(y - x for x, y in predicted_intervals),\n",
    "    )\n",
    "\n",
    "    result_dict.update(dict(\n",
    "        metr_predicted_no_match_total = result_dict['metr_predicted_total'] - result_dict['metr_matched_total'],\n",
    "        metr_true_no_prediction_total = result_dict['metr_true_area_total'] - result_dict['metr_matched_total'],\n",
    "    ))\n",
    "\n",
    "    result_dict['abs_true_matched_per_project'] = []\n",
    "    result_dict['perc_true_matched_per_project'] = []\n",
    "    result_dict['project_length'] = []\n",
    "    for actual in actual_intervals:\n",
    "        result_dict['abs_true_matched_per_project'].append(sum((interval[1] - interval[0]) for interval in actual_intersection[actual]) if actual in actual_intersection else 0)\n",
    "        result_dict['perc_true_matched_per_project'].append(result_dict['abs_true_matched_per_project'][-1] / (actual[1] - actual[0]))\n",
    "        result_dict['project_length'].append(actual[1] - actual[0])\n",
    "\n",
    "    result_dict['abs_predicted_matched_per_predicted_project'] = []\n",
    "    result_dict['perc_predicted_matched_per_predicted_project'] = []\n",
    "    for predicted in predicted_intervals:\n",
    "        result_dict['abs_predicted_matched_per_predicted_project'].append(sum((interval[1] - interval[0]) for interval in predicted_intersection[predicted]) if predicted in predicted_intersection else 0)\n",
    "        result_dict['perc_predicted_matched_per_predicted_project'].append(result_dict['abs_predicted_matched_per_predicted_project'][-1] / (predicted[1] - predicted[0]))\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def make_quant_comparison(\n",
    "    projects: pd.DataFrame, \n",
    "    clustered_projects: pd.DataFrame, \n",
    "    index_df: pd.DataFrame,\n",
    "    prediction_cols: pd.Index, \n",
    "    h_index: Tuple=['RoadID', 'Direction'],\n",
    "    mask: pd.DataFrame=None, \n",
    "    invalid_labels: set=set(),\n",
    "):\n",
    "\n",
    "    results = {\n",
    "        treatment: {\n",
    "            treatment_time: [] for treatment_time in set(prediction_cols.get_level_values(0))\n",
    "        } for treatment in set(prediction_cols.get_level_values(1))\n",
    "    }\n",
    "\n",
    "    index_lst = list(index_df[h_index].drop_duplicates().itertuples(index=False, name=None))\n",
    "\n",
    "    tqdm_prediction = tqdm(desc='Prediction columns', total=len(prediction_cols), leave=False)\n",
    "    tqdm_index_lst = tqdm(desc='Index', total=len(index_lst), leave=False)\n",
    "\n",
    "    tqdm_prediction.reset()\n",
    "    for (treatment_time, treatment) in prediction_cols:\n",
    "        tqdm_prediction.update()\n",
    "        tqdm_index_lst.reset()\n",
    "        for index in index_lst:\n",
    "            tqdm_index_lst.update()\n",
    "            if len(invalid_labels) > 0 and (*index, treatment_time, treatment) in invalid_labels:\n",
    "                continue\n",
    "\n",
    "            actual_df = projects[\n",
    "                (projects[h_index].values == index).all(axis=1) &\\\n",
    "                (projects['Treatment'] == treatment) &\\\n",
    "                (projects[treatment_time] == True)\n",
    "            ].copy()\n",
    "            predicted_df = clustered_projects[\n",
    "                (clustered_projects[h_index].values == index).all(axis=1) &\\\n",
    "                (clustered_projects['Treatment'] == treatment) &\\\n",
    "                (clustered_projects['Treatment Time'] == treatment_time)\n",
    "            ].copy()\n",
    "    \n",
    "            # clip invalid regions \n",
    "            if mask is not None and len(mask) > 0:\n",
    "                subset = mask[(mask[h_index].values == index).all(axis=1)]\n",
    "                if len(subset) > 0:\n",
    "                    # ignore true treatments where labels have no entries; improve result over what no algorithm can do better, since labels are missing anyway\n",
    "                    for _, row in subset.iterrows():\n",
    "                        actual_df = clip_mask(row['Start'], row['End'], actual_df)\n",
    "                        predicted_df = clip_mask(row['Start'], row['End'], predicted_df)\n",
    "            \n",
    "            if len(actual_df) == 0:\n",
    "                continue\n",
    "     \n",
    "            r = calculate_compare_projects(\n",
    "                actual_df=actual_df, predicted_df=predicted_df,\n",
    "                h_index=h_index,\n",
    "            )\n",
    "\n",
    "            r['index'] = index\n",
    "            results[treatment][treatment_time].append(r)\n",
    "\n",
    "        results[treatment][treatment_time] = pd.DataFrame(results[treatment][treatment_time])\n",
    "\n",
    "    tqdm_index_lst.close()\n",
    "    tqdm_prediction.close()\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method plotting quantiative comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "# code from https://linuxtut.com/en/92c21048bacadce811ec/\n",
    "def set_hierarchical_xlabels(index, ax=None,\n",
    "                             bar_xmargin=0.1, #Margins on the left and right ends of the line, X-axis scale\n",
    "                             bar_yinterval=0.1, #Relative value with the vertical spacing of the line and the length of the Y axis as 1?\n",
    "                            ):\n",
    "    from itertools import groupby\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    assert isinstance(index, pd.MultiIndex)\n",
    "    labels = ax.set_xticklabels([s for *_, s in index])\n",
    "    for lb in labels:\n",
    "        lb.set_rotation(0)\n",
    "\n",
    "    transform = ax.get_xaxis_transform()\n",
    "\n",
    "    for i in range(1, len(index.codes)):\n",
    "        xpos0 = -0.5 #Coordinates on the left side of the target group\n",
    "        for (*_, code), codes_iter in groupby(zip(*index.codes[:-i])):\n",
    "            xpos1 = xpos0 + sum(1 for _ in codes_iter) #Coordinates on the right side of the target group\n",
    "            ax.text((xpos0+xpos1)/2, (bar_yinterval * (-i-0.1)),\n",
    "                    index.levels[-i-1][code],\n",
    "                    transform=transform,\n",
    "                    ha=\"center\", va=\"top\")\n",
    "            ax.add_line(Line2D([xpos0+bar_xmargin, xpos1-bar_xmargin],\n",
    "                               [bar_yinterval * -i]*2,\n",
    "                               transform=transform,\n",
    "                               color=\"k\", clip_on=False))\n",
    "            xpos0 = xpos1\n",
    "\n",
    "def plot_quant_comparison(quant_results: dict, axs: List[plt.Axes]=None, ylim: tuple=None):\n",
    "\n",
    "    # preprocess quant_results\n",
    "    plot_df = pd.DataFrame({'Treatment': [], 'Treatment Time': [], 'Metric': [], 'Value': [], 'Error': []})\n",
    "\n",
    "    for treatment, treatment_dict in quant_results.items():\n",
    "        if treatment not in ['Resurfacing_SS', 'Rehabilitation', 'Resurfacing_AC']:\n",
    "            continue\n",
    "        for treatment_time, result_df in treatment_dict.items():\n",
    "            if (treatment_time == 'Treatment between 10 to 30 years') or (len(result_df) == 0):\n",
    "                continue\n",
    "\n",
    "            # ratio of total length of accurately predicted treatment over total length of all project\n",
    "            recall_row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': r'Recall by length ($\\frac{overlap}{actual})$'}\n",
    "            recall_row['Value'] = np.sum(result_df['metr_matched_total']) / np.sum(result_df['metr_true_area_total'])\n",
    "            recall_row['Error'] = 0\n",
    "\n",
    "            # ratio of total length of accurately predicted treatment over total length of all predicted project\n",
    "            precision_row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': r'Precision by length ($\\frac{overlap}{predicted}$)'}\n",
    "            precision_row['Value'] = np.sum(result_df['metr_matched_total']) / np.sum(result_df['metr_predicted_total'])\n",
    "            precision_row['Error'] = 0\n",
    "\n",
    "            # percentage_of_actual_planned_project_predicted needs to be merged \n",
    "            perc_row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': 'Average percentage of actual project predicted'}\n",
    "            all_percs = np.array(list(chain.from_iterable(result_df['perc_true_matched_per_project'].to_list())))\n",
    "            perc_row['Value'] = np.mean(all_percs)\n",
    "            perc_row['Error'] = 0 \n",
    "\n",
    "            # count_of_actual_projects_with_more_than_30\n",
    "            row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': 'Percentage of actual projects with more than 30% predicted in length'}\n",
    "            all_lengths = np.array(list(chain.from_iterable(result_df['project_length'].to_list())))\n",
    "            more_than_threshold = all_percs >= 0.3\n",
    "            row['Value'] = np.sum(more_than_threshold) / len(all_lengths) \n",
    "            row['Error'] = 0\n",
    "\n",
    "            # plot_df = plot_df.append(metr_row, ignore_index=True)\n",
    "            plot_df = plot_df.append(perc_row, ignore_index=True)\n",
    "            plot_df = plot_df.append(row, ignore_index=True)\n",
    "            plot_df = plot_df.append(recall_row, ignore_index=True)\n",
    "            plot_df = plot_df.append(precision_row, ignore_index=True)\n",
    "\n",
    "    plot_df = plot_df.sort_values(by=['Treatment', 'Treatment Time'], \n",
    "        key=lambda x: x.replace(year_order_dict) if x.name == 'Treatment Time' else x.replace(treatment_type_order)\n",
    "    )\n",
    "    plot_df.loc[:, 'Treatment Time'] = plot_df['Treatment Time'].replace(year_map_dict)\n",
    "\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(nrows=plot_df['Metric'].nunique(), ncols=1, figsize=(12, 4 * plot_df['Metric'].nunique()))\n",
    "\n",
    "    for i, metric in enumerate(plot_df['Metric'].unique()):\n",
    "        inner_df = plot_df[plot_df['Metric'] == metric].set_index(['Treatment', 'Treatment Time'])\n",
    "        inner_df.plot(\n",
    "            kind='bar',\n",
    "            y='Value',\n",
    "            yerr='Error',\n",
    "            ax=axs[i],\n",
    "            xlabel=None,\n",
    "            color=[treatment_type_colors[treatment] for treatment, _ in inner_df.index],\n",
    "            alpha=0.6,\n",
    "            legend=False,\n",
    "        )\n",
    "        set_hierarchical_xlabels(inner_df.index, ax=axs[i])\n",
    "        axs[i].set_title(metric)\n",
    "        axs[i].set_xlabel(None)\n",
    "        axs[i].set_ylabel('Fraction')\n",
    "        if axs[i].get_ylim()[1] < 1:\n",
    "            axs[i].set_ylim((0, 1))\n",
    "        axs[i].grid(True)\n",
    "\n",
    "        if ylim is not None:\n",
    "            axs[metric].set_ylim(ylim)\n",
    "\n",
    "    return axs, plot_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define algorithms\n",
    "\n",
    "We expect the validation set to occupy every two road section for any particular road id. Thus, we will fill the immediate predecessor and successor of a predicted treatment section with the same treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Linear scan\n",
    "\n",
    "Given hierarchical spatial index (RoadID and Direction), scan linearly for continous spatial index (chainage as defined by start_end) and groups consecutive scans of specific treatment into the projects.\n",
    "\n",
    "This method merely use the merge_intervals helper method defined previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_scan(sorted_index: pd.DataFrame, nudge_outer: bool=True, **kwargs):\n",
    "    sorted_intervals = list(sorted_index[['Start', 'End']].itertuples(index=False, name=None))\n",
    "\n",
    "    if nudge_outer:\n",
    "        sorted_intervals = [(max(a - 100, 0), b + 100) for a, b in sorted_intervals]\n",
    "    return merge_intervals(sorted_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Density Based Clustering\n",
    "\n",
    "Use DBSCAN algorithm on all predicted sections (after merging consecutive runs where possible), with the 'distance' metric between any two predictions being the distance between their edges in meters.\n",
    "\n",
    "Parameters:\n",
    "-  `eps`, `min_samples`. Used in DBSCAN algorithm as documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN). Intuitively, a clustered project greedily expands itself until no elementary project remains within `eps` meters from either edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def cluster(sorted_index: pd.DataFrame, nudge_outer: bool=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Cluster groups of smaller projects into a larger project if they are close together.\n",
    "    Underlying algorithm\n",
    "\n",
    "    :params:\n",
    "        eps, min_samples: used in DBSCAN algorithm. \n",
    "    \n",
    "    \"\"\"\n",
    "    elementary_projects = linear_scan(sorted_index, nudge_outer)\n",
    "\n",
    "    distance_mat = pairwise_distances(np.array(elementary_projects), metric=lambda x, y: max(y[0] - x[1], x[0] - y[1]), n_jobs=3 if len(elementary_projects) > 10000 else 1)\n",
    "    np.fill_diagonal(distance_mat, 0) # distance between same row not 0 by lambda method\n",
    "\n",
    "    clusterer = DBSCAN(\n",
    "        eps=2000 if 'eps' not in kwargs else kwargs['eps'], \n",
    "        min_samples=1 if 'min_samples' not in kwargs else kwargs['min_samples'],\n",
    "        metric='precomputed',\n",
    "    )\n",
    "    clustering_result = clusterer.fit_predict(distance_mat)\n",
    "\n",
    "    # groupby clustering results\n",
    "    projects = pd.DataFrame(elementary_projects, columns=['Start', 'End'])\n",
    "    projects['cluster_labels'] = clustering_result\n",
    "\n",
    "    # remove noise\n",
    "    projects = projects[projects['cluster_labels'] != -1]\n",
    "\n",
    "    projects = projects.groupby(['cluster_labels']).agg({'Start': 'min', 'End': 'max'})\n",
    "    sorted_intervals = list(projects[['Start', 'End']].itertuples(index=False, name=None))\n",
    "\n",
    "    return merge_intervals(sorted_intervals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {\n",
    "    juri: dict(\n",
    "        clustered_projects=[],\n",
    "    ) for juri in ['WA', 'NZ', 'VIC']\n",
    "}\n",
    "\n",
    "plot_dict = {juri: {} for juri in ['WA', 'NZ', 'VIC']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for juri in tqdm(['WA', 'NZ', 'VIC'], desc='Juri'):\n",
    "    predictions = np.array([\n",
    "        model.predict(input_dict[juri]['features']) for model in input_dict[juri]['models']\n",
    "    ]).mean(axis=0)\n",
    "    predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "    predictions = pd.DataFrame(predictions, columns=input_dict[juri]['prediction_cols'])\n",
    "    index_df = input_dict[juri]['index']\n",
    "\n",
    "    for index_tup in tqdm(index_df[input_dict[juri]['h_index']].drop_duplicates().itertuples(index=False, name=None), desc='Spatial Index', total=index_df[input_dict[juri]['h_index']].drop_duplicates().shape[0]):\n",
    "        # filter empty patches\n",
    "        for treatment_idx, (treatment_time, treatment) in enumerate(input_dict[juri]['prediction_cols']):\n",
    "            filtered_index = index_df[\n",
    "                (np.all(index_df[input_dict[juri]['h_index']].values == index_tup, axis=1)) &\\\n",
    "                (predictions[(treatment_time, treatment)] == 1)\n",
    "            ]\n",
    "            sorted_index = filtered_index.sort_values(['Start'])\n",
    "            if len(sorted_index) == 0:\n",
    "                continue\n",
    "                \n",
    "            # projects_intervals = linear_scan(sorted_index)\n",
    "            projects_intervals = cluster(sorted_index, nudge_outer=True)\n",
    "\n",
    "            for start_chainage, end_chainage in projects_intervals:\n",
    "                row = {\n",
    "                    'Treatment': treatment, \n",
    "                    'Treatment Time': treatment_time, \n",
    "                    'Start': start_chainage,\n",
    "                    'End': end_chainage,\n",
    "                }\n",
    "                row.update({k: v for k, v in zip(input_dict[juri]['h_index'], index_tup)})\n",
    "                result_dict[juri]['clustered_projects'].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mask for index and labels\n",
    "# mask on both train and valid index, as we want to include train index into our final metrics without performing computations explictily on them\n",
    "for juri in ['VIC', 'WA', 'NZ']:\n",
    "    result_dict[juri]['index_mask'] = find_mask_ranges(\n",
    "        input_dict[juri]['projects'], \n",
    "        pd.concat([input_dict[juri]['index'], \n",
    "            wa_train_index if juri == 'WA' else\\\n",
    "            (nz_train_index if juri == 'NZ' else vic_train_index)\n",
    "        ], ignore_index=True, axis=0), \n",
    "        h_index=input_dict[juri]['h_index']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MASK = True \n",
    "\n",
    "for juri, val_dict in tqdm(result_dict.items(), desc='juri'):\n",
    "    results = make_quant_comparison(\n",
    "        input_dict[juri]['projects'], \n",
    "        pd.DataFrame(val_dict['clustered_projects']), \n",
    "        input_dict[juri]['index'],\n",
    "        input_dict[juri]['prediction_cols'], \n",
    "        h_index=input_dict[juri]['h_index'],\n",
    "        mask=val_dict['index_mask'] if USE_MASK else None\n",
    "    )\n",
    "    plot_dict[juri] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MASK = True \n",
    "\n",
    "with open(REPORT_DIR / f'raw_plot_dict_{SUFFIX}_{\"masked\" if USE_MASK else \"unmasked\"}.pkl', 'wb') as f:\n",
    "    pickle.dump(plot_dict, f)\n",
    "\n",
    "with open(REPORT_DIR / f'raw_result_dict_{SUFFIX}.pkl', 'wb') as f:\n",
    "    pickle.dump(result_dict, f)\n",
    "\n",
    "with open(REPORT_DIR / f'raw_plot_dict_{SUFFIX}_{\"masked\" if USE_MASK else \"unmasked\"}.pkl', 'rb') as f:\n",
    "    plot_dict = pickle.load(f)\n",
    "\n",
    "with open(REPORT_DIR / f'raw_result_dict_{SUFFIX}.pkl', 'rb') as f:\n",
    "    result_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for juri, val_dict in tqdm(plot_dict.items()):\n",
    "    axs, plot_dict_cumulative = plot_quant_comparison(val_dict)\n",
    "    for ax in axs:\n",
    "        ax.xaxis.grid(False)\n",
    "    fig = axs[0].get_figure()\n",
    "    fig.suptitle(f'Performance on whole road network by clustering predictions on validation set of short sections.\\n{juri} - {\" \".join(map(str.capitalize, SUFFIX.split(\"_\")))}')\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    fig.savefig(REPORT_DIR / f'{juri.lower()}_cluster_level_perf_{SUFFIX}_{\"masked\" if USE_MASK else \"unmasked\"}.png', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juri = 'WA'\n",
    "treatment_tup = ('Treatment within 1 year', 'Resurfacing_SS')\n",
    "\n",
    "juri_predictions = np.array([\n",
    "    model.predict(input_dict[juri]['features']) for model in input_dict[juri]['models']\n",
    "]).mean(axis=0)\n",
    "juri_predictions = np.where(juri_predictions >= 0.5, 1, 0)\n",
    "juri_predictions = pd.DataFrame(juri_predictions, columns=input_dict[juri]['prediction_cols'])\n",
    "juri_index_df = input_dict[juri]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mask for index and labels\n",
    "plot_mask = {}\n",
    "for juri in ['WA', 'NZ']:\n",
    "    plot_mask[juri] = find_mask_ranges(\n",
    "        input_dict[juri]['projects'], \n",
    "        input_dict[juri]['index'],\n",
    "        h_index=input_dict[juri]['h_index']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juri = 'WA'\n",
    "x = plot_dict['WA']['Resurfacing_SS']['Treatment within 1 year']\n",
    "x[x['index'] == ('H005', 'S')]\n",
    "index_tup = ('H005', 'S')\n",
    "h_index = input_dict[juri]['h_index']\n",
    "\n",
    "axs, stacked_split = plot_compare_projects(\n",
    "    actual_df=input_dict[juri]['projects'][\n",
    "        np.all(input_dict[juri]['projects'][h_index].values == index_tup, axis=1) &\\\n",
    "        (input_dict[juri]['projects']['Treatment'] == treatment_tup[1]) &\\\n",
    "        (input_dict[juri]['projects'][treatment_tup[0]] == True)\n",
    "    ],\n",
    "    predicted_df=pd.DataFrame(result_dict[juri]['clustered_projects'])[\n",
    "        np.all(pd.DataFrame(result_dict[juri]['clustered_projects'])[h_index].values == index_tup, axis=1) &\\\n",
    "        np.all(pd.DataFrame(result_dict[juri]['clustered_projects'])[['Treatment Time', 'Treatment']].values == treatment_tup, axis=1)\n",
    "    ],\n",
    "    unclustered_df=juri_index_df.loc[\n",
    "        pd.Series(np.all(juri_index_df[h_index].values == index_tup, axis=1)) &\\\n",
    "        (juri_predictions[treatment_tup] == 1)\n",
    "    ],\n",
    "    h_index=h_index,\n",
    "    index_mask=plot_mask[juri][\n",
    "        np.all(plot_mask[juri][h_index].values == index_tup, axis=1)\n",
    "    ]\n",
    ") \n",
    "\n",
    "for ax in axs:\n",
    "    step = 500\n",
    "    xticks = np.arange(ax.get_xlim()[0], ax.get_xlim()[1], step=step)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([f'{step * x}' for x in range(len(xticks))])\n",
    "\n",
    "fig = axs[0].get_figure()\n",
    "\n",
    "fig.suptitle(\"Example comparison of clustered and ground truth projects\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(REPORT_DIR / f'example_{SUFFIX}.png', dpi=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare recall and precision by length vs recall and precision by section prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load valid result\n",
    "suffix = SUFFIX \n",
    "from src import DATA_DIR\n",
    "\n",
    "report_dir = DATA_DIR.parent / 'reports' / 'raw_results'\n",
    "model_dir = DATA_DIR.parent / 'models' / 'trained'\n",
    "\n",
    "paths_dict = { \n",
    "    'conf_mat': {\n",
    "        'WA': report_dir / 'MRWA' / f'mrwa_final_{suffix}_dir' / f'valid_XGB_rawconfmat_mrwa_final_{suffix}.pkl',\n",
    "        'NZ': report_dir / 'NZTA' / f'nzta_final_{suffix}_dir' / f'valid_XGB_rawconfmat_nzta_final_{suffix}.pkl',\n",
    "        'VIC': report_dir / 'VIC' / f'vic_final_{suffix}_dir' / f'valid_XGB_rawconfmat_vic_final_{suffix}.pkl',\n",
    "    },\n",
    "    'prediction_columns': {\n",
    "        'WA': model_dir / 'MRWA' / f'mrwa_final_{suffix}_dir' / f'train_labels_columns_mrwa_final_{suffix}.pkl',\n",
    "        'NZ': model_dir / 'NZTA' / f'nzta_final_{suffix}_dir' / f'train_labels_columns_nzta_final_{suffix}.pkl',\n",
    "        'VIC': model_dir / 'VIC' / f'vic_final_{suffix}_dir' / f'train_labels_columns_vic_final_{suffix}.pkl',\n",
    "    }\n",
    "}\n",
    "\n",
    "metric_dict = {}\n",
    "for juri, running_conf_mat_path in paths_dict['conf_mat'].items():\n",
    "    with open(paths_dict['prediction_columns'][juri], 'rb') as f:\n",
    "        pred_cols = pickle.load(f)\n",
    "    with open(running_conf_mat_path, 'rb') as f:\n",
    "        running_conf_mat_arr = np.array(pickle.load(f))\n",
    "    metric_dict[juri] = {}\n",
    "    for i, (time_type, treatment_type) in enumerate(pred_cols):\n",
    "        running_conf_mat = running_conf_mat_arr[:, i, :, :]\n",
    "\n",
    "        if treatment_type not in metric_dict[juri]: metric_dict[juri][treatment_type] = {}\n",
    "        if time_type not in metric_dict[juri][treatment_type]: metric_dict[juri][treatment_type][time_type] = {}\n",
    "        precision = (running_conf_mat[:, 1, 1] / running_conf_mat[:, :, 1].sum(axis=1))\n",
    "        recall = (running_conf_mat[:, 1, 1] / running_conf_mat[:, 1, :].sum(axis=1))\n",
    "\n",
    "        metric_dict[juri][treatment_type][time_type]['Precision'] = np.mean(precision)\n",
    "        metric_dict[juri][treatment_type][time_type]['Recall'] = np.mean(recall)\n",
    "        metric_dict[juri][treatment_type][time_type]['F-Score'] = np.mean(2 / (1 / precision + 1 / recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_recall_precision_by_length_by_section(juri: str, project_plot_dict: dict, section_metric_dict: dict, axs=None, ylim=None, cmap=None):\n",
    "    \"\"\"Compare precision and recall by length vs precision and recall by section classification results\"\"\"\n",
    "\n",
    "    plot_df = []\n",
    "    for treatment, treatment_dict in project_plot_dict[juri].items():\n",
    "        if treatment not in ['Resurfacing_SS', 'Resurfacing_AC', 'Rehabilitation']:\n",
    "            continue\n",
    "        for treatment_time, result_df in treatment_dict.items():\n",
    "            if (treatment_time == 'Treatment between 10 to 30 years') or (len(result_df) == 0):\n",
    "                continue\n",
    "\n",
    "            # ratio of total length of accurately predicted treatment over total length of all project\n",
    "            recall_row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': 'Recall', 'By': 'project-level'}\n",
    "            recall_row['Value'] = np.sum(result_df['metr_matched_total']) / np.sum(result_df['metr_true_area_total'])\n",
    "            recall_row['Error'] = 0\n",
    "            # ratio of total length of accurately predicted treatment over total length of all predicted project\n",
    "            precision_row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': 'Precision', 'By': 'project-level'}\n",
    "            precision_row['Value'] = np.sum(result_df['metr_matched_total']) / np.sum(result_df['metr_predicted_total'])\n",
    "            precision_row['Error'] = 0\n",
    "\n",
    "            plot_df.append(recall_row)\n",
    "            plot_df.append(precision_row)\n",
    "    \n",
    "    for treatment, treatment_dict in section_metric_dict[juri].items():\n",
    "        for treatment_time, result_df in treatment_dict.items():\n",
    "            if (treatment_time == 'Treatment between 10 to 30 years') or (len(result_df) == 0):\n",
    "                continue\n",
    "\n",
    "            # ratio of total length of accurately predicted treatment over total length of all project\n",
    "            recall_row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': 'Recall', 'By': 'section-level'}\n",
    "            recall_row['Value'] = result_df['Recall'] \n",
    "            recall_row['Error'] = 0\n",
    "\n",
    "            # ratio of total length of accurately predicted treatment over total length of all predicted project\n",
    "            precision_row = {'Treatment': treatment, 'Treatment Time': treatment_time, 'Metric': 'Precision', 'By': 'section-level'}\n",
    "            precision_row['Value'] = result_df['Precision'] \n",
    "            precision_row['Error'] = 0\n",
    "\n",
    "            plot_df.append(recall_row)\n",
    "            plot_df.append(precision_row)\n",
    "\n",
    "    def key(series: pd.Series):\n",
    "        if series.name == 'Treatment': return series.replace(year_order_dict)\n",
    "        if series.name == 'Treatment Time': return series.replace(treatment_type_order)\n",
    "        if series.name == 'By': return series.replace({'projecrt-level': 0, 'section-level': 1})\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_df)\n",
    "    plot_df.loc[:, 'Treatment Time'] = plot_df['Treatment Time'].replace(year_map_dict)\n",
    "    plot_df : pd.DataFrame = plot_df.sort_values(by=['Treatment', 'Treatment Time', 'By'], \n",
    "        key=key\n",
    "    )\n",
    "\n",
    "    # remove treatment, treatment_time if do not have both project-section level\n",
    "    count_by = plot_df.groupby(['Treatment', 'Treatment Time', 'Metric'])['By'].transform('nunique')\n",
    "    plot_df = plot_df[count_by > 1]\n",
    "\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(nrows=plot_df['Metric'].nunique(), ncols=1, figsize=(12, 5 * plot_df['Metric'].nunique()))\n",
    "\n",
    "    for i, metric in enumerate(plot_df['Metric'].unique()):\n",
    "        inner_df = plot_df.loc[plot_df['Metric'] == metric, :]\\\n",
    "                .pivot(index=['Treatment', 'Treatment Time'], columns='By', values='Value')\n",
    "        inner_df.plot(\n",
    "            kind='bar',\n",
    "            ax=axs[i],\n",
    "            xlabel=False,\n",
    "            legend=False,\n",
    "            cmap='tab10' if cmap is None else cmap\n",
    "        )\n",
    "\n",
    "        # set xticks between any 2 bars\n",
    "        set_hierarchical_xlabels(inner_df.index, ax=axs[i])\n",
    "        axs[i].set_title((f'Jurisdiction: {juri}\\nMethod: {\" \".join(map(str.capitalize, SUFFIX.split(\"_\")))}\\n\\n' if i == 0 else '') + metric + \" by project level (using length) v.s. by section-level (using prediction)\", loc='left')\n",
    "        axs[i].set_xlabel(None)\n",
    "        axs[i].set_ylabel('Fraction')\n",
    "        axs[i].legend(loc='upper left')\n",
    "        if axs[i].get_ylim()[1] < 1:\n",
    "            axs[i].set_ylim((0, 1))\n",
    "\n",
    "        if ylim is not None:\n",
    "            axs[metric].set_ylim(ylim)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = mpl_colors.ListedColormap(['#00729B', '#004259', '#002D3D', '#000F14'], name='austroads_cmap') # color correspond to flag index\n",
    "\n",
    "plot_compare_recall_precision_by_length_by_section('NZ', plot_dict, metric_dict, cmap=colors)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR / 'NZ_compare_by_length_by_prediction_{}.png'.format(SUFFIX))\n",
    "\n",
    "plot_compare_recall_precision_by_length_by_section('WA', plot_dict, metric_dict, cmap=colors)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR / 'WA_compare_by_length_by_prediction_{}.png'.format(SUFFIX))\n",
    "\n",
    "plot_compare_recall_precision_by_length_by_section('VIC', plot_dict, metric_dict, cmap=colors)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR / 'VIC_compare_by_length_by_prediction_{}.png'.format(SUFFIX))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
