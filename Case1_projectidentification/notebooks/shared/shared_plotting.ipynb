{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from src import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DATA_DIR\n",
    "import pickle\n",
    "report_dir = DATA_DIR.parent / 'reports' / 'raw_results'\n",
    "model_dir = DATA_DIR.parent / 'models' / 'trained'\n",
    "SUFFIX = 'even_split'\n",
    "suffix = SUFFIX\n",
    "\n",
    "paths_dict = { \n",
    "    'conf_mat': {\n",
    "        'WA': report_dir / 'MRWA' / f'mrwa_final_{suffix}_dir' / f'valid_XGB_rawconfmat_mrwa_final_{suffix}.pkl',\n",
    "        'NZ': report_dir / 'NZTA' / f'nzta_final_{suffix}_dir' / f'valid_XGB_rawconfmat_nzta_final_{suffix}.pkl',\n",
    "        'NSW': report_dir / 'NSW' / f'nsw_final_{suffix}_dir' / f'valid_XGB_rawconfmat_nsw_final_{suffix}.pkl',\n",
    "        'VIC': report_dir / 'VIC' / f'vic_final_{suffix}_dir' / f'valid_XGB_rawconfmat_vic_final_{suffix}.pkl',\n",
    "    },\n",
    "    'conf_mat_dummy': {\n",
    "        'WA': report_dir / 'MRWA' / f'mrwa_final_{suffix}_dir' / f'valid_dummy_rawconfmat_mrwa_final_{suffix}.pkl',\n",
    "        'NZ': report_dir / 'NZTA' / f'nzta_final_{suffix}_dir' / f'valid_dummy_rawconfmat_nzta_final_{suffix}.pkl',\n",
    "        'NSW': report_dir / 'NSW' / f'nsw_final_{suffix}_dir' / f'valid_dummy_rawconfmat_nsw_final_{suffix}.pkl',\n",
    "        'VIC': report_dir / 'VIC' / f'vic_final_{suffix}_dir' / f'valid_dummy_rawconfmat_vic_final_{suffix}.pkl',\n",
    "    },\n",
    "    'prediction_columns': {\n",
    "        'WA': model_dir / 'MRWA' / f'mrwa_final_{suffix}_dir' / f'train_labels_columns_mrwa_final_{suffix}.pkl',\n",
    "        'NZ': model_dir / 'NZTA' / f'nzta_final_{suffix}_dir' / f'train_labels_columns_nzta_final_{suffix}.pkl',\n",
    "        'NSW': model_dir / 'NSW' / f'nsw_final_{suffix}_dir' / f'train_labels_columns_nsw_final_{suffix}.pkl',\n",
    "        'VIC': model_dir / 'VIC' / f'vic_final_{suffix}_dir' / f'train_labels_columns_vic_final_{suffix}.pkl',\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    juri: {\n",
    "        'models': {\n",
    "            'XGB': None\n",
    "        },\n",
    "        'prediction_columns': None\n",
    "    } for juri in ['NSW', 'VIC', 'WA', 'NZ']\n",
    "}\n",
    "\n",
    "juris = ['NSW', 'MRWA', 'NZTA']\n",
    "prefixes = ['final'] * 3\n",
    "suffixes = [suffix] * 3\n",
    "\n",
    "for train_name, prefix, suffix in zip(juris, prefixes, suffixes):\n",
    "    juri_name = train_name.replace('TA', '').replace('MR', '') # turn mrwa->wa and nzta->nz\n",
    "    model_dir = DATA_DIR.parent / 'models' / 'trained' / train_name / f'{train_name.lower()}_{prefix}_{suffix}_dir'\n",
    "    for model_type in ['XGB']:\n",
    "        with open(model_dir / f'train_{model_type}_timehorizon_{train_name.lower()}_{prefix}_{suffix}.pkl', 'rb') as f:\n",
    "            model_dict[juri_name]['models'][model_type] = pickle.load(f)\n",
    "    with open(model_dir  / f'train_labels_columns_{train_name.lower()}_{prefix}_{suffix}.pkl', 'rb') as f:\n",
    "        model_dict[juri_name]['prediction_columns'] = pickle.load(f)\n",
    "\n",
    "result_dict = {}\n",
    "for juri in ['WA', 'NSW', 'NZ', 'VIC']:\n",
    "    result_dict[juri] = {}\n",
    "    for val_type in ['conf_mat', 'prediction_columns', 'conf_mat_dummy']:\n",
    "        pth = paths_dict[val_type][juri]\n",
    "        with open(pth, 'rb') as f:\n",
    "            val = pickle.load(f)\n",
    "            result_dict[juri][val_type] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import util\n",
    "train_flattened_mrwa_labels = util.load_data(DATA_DIR / 'processed' / 'MRWA' / 'mrwa_final' / 'train_flattened_labels_mrwa_final.csv', header=[0, 1])\n",
    "train_flattened_nzta_labels = util.load_data(DATA_DIR / 'processed' / 'NZTA' / 'nzta_final' / 'train_flattened_labels_nzta_final.csv', header=[0, 1])\n",
    "valid_flattened_mrwa_labels = util.load_data(DATA_DIR / 'processed' / 'MRWA' / 'mrwa_final' / 'valid_flattened_labels_mrwa_final.csv', header=[0, 1])\n",
    "valid_flattened_nzta_labels = util.load_data(DATA_DIR / 'processed' / 'NZTA' / 'nzta_final' / 'valid_flattened_labels_nzta_final.csv', header=[0, 1])\n",
    "\n",
    "train_flattened_mrwa = util.load_data(DATA_DIR / 'processed' / 'MRWA' / 'mrwa_final' / 'train_flattened_data_mrwa_final_no_offset.csv')\n",
    "train_flattened_nzta = util.load_data(DATA_DIR / 'processed' / 'NZTA' / 'nzta_final' / 'train_flattened_data_nzta_final_no_offset.csv')\n",
    "valid_flattened_mrwa = util.load_data(DATA_DIR / 'processed' / 'MRWA' / 'mrwa_final' / 'valid_flattened_data_mrwa_final_no_offset.csv')\n",
    "valid_flattened_nzta = util.load_data(DATA_DIR / 'processed' / 'NZTA' / 'nzta_final' / 'valid_flattened_data_nzta_final_no_offset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flattened_nsw = util.load_data(DATA_DIR / 'processed' / 'NSW' / 'final' / 'train_all.csv')\n",
    "train_flattened_nsw_labels = util.load_data(DATA_DIR / 'processed' / 'NSW' / 'final' / 'labels_all.csv', header=[0, 1])\n",
    "train_flattened_vic = util.load_data(DATA_DIR / 'processed' / 'VIC' / 'final' / 'train_all.csv')\n",
    "train_flattened_vic_labels = util.load_data(DATA_DIR / 'processed' / 'VIC' / 'final' / 'labels_all.csv', header=[0, 1])\n",
    "\n",
    "valid_flattened_nsw = util.load_data(DATA_DIR / 'processed' / 'NSW' / 'final' / 'valid_all.csv')\n",
    "valid_flattened_nsw_labels = util.load_data(DATA_DIR / 'processed' / 'NSW' / 'final' / 'valid_labels_all.csv', header=[0, 1])\n",
    "valid_flattened_vic = util.load_data(DATA_DIR / 'processed' / 'VIC' / 'final' / 'valid_all.csv')\n",
    "valid_flattened_vic_labels = util.load_data(DATA_DIR / 'processed' / 'VIC' / 'final' / 'valid_labels_all.csv', header=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_map_dict = {\n",
    "    'Treatment within 1 year': 'Year 1',\n",
    "    'Treatment between 1 to 3 years': 'Year 2 - 3',\n",
    "    'Treatment between 3 to 5 years': 'Year 4 - 5',\n",
    "    'Treatment between 5 to 10 years': 'Year 6 - 10',\n",
    "}\n",
    "\n",
    "year_order_dict = {\n",
    "    'Treatment within 1 year': 0,\n",
    "    'Treatment between 1 to 3 years': 1, \n",
    "    'Treatment between 3 to 5 years': 2, \n",
    "    'Treatment between 5 to 10 years': 3,\n",
    "}\n",
    "\n",
    "treatment_type_order = {\n",
    "    'Resurfacing_SS': 0,\n",
    "    'Resurfacing_AC': 1,\n",
    "    'Major Patching': 2,\n",
    "    'Rehabilitation': 3,\n",
    "    'Retexturing': 4,\n",
    "    'Regulation': 5\n",
    "}\n",
    "\n",
    "treatment_time_order = {\n",
    "    'Treatment within 1 year': 0,\n",
    "    'Treatment between 1 to 3 years': 1,\n",
    "    'Treatment between 3 to 5 years': 2,\n",
    "    'Treatment between 5 to 10 years': 3,\n",
    "    'Treatment between 10 to 30 years': 4\n",
    "}\n",
    "\n",
    "juri_order = {juri: i for i, juri in enumerate(['WA', 'NSW', 'VIC', 'NZ'])}\n",
    "juri_colors = {'WA': 'tab:blue', 'NSW': 'tab:orange', 'VIC': 'tab:green', 'NZ': 'tab:red'}\n",
    "\n",
    "treatment_type_colors = {\n",
    "    'Resurfacing_SS': 'tab:purple',\n",
    "    'Resurfacing_AC': 'tab:brown',\n",
    "    'Major Patching': 'tab:gray',\n",
    "    'Rehabilitation': 'tab:olive',\n",
    "    'Retexturing': 'tab:cyan',\n",
    "    'Regulation': 'tab:pink',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_dir = report_dir.parent / 'figures' / 'shared' \n",
    "if save_fig_dir.exists() is False:\n",
    "    save_fig_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation plots\n",
    "Plot correlation within datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_correlation(corr, title, figsize=(16, 8), heatmap_dict={}, colormap=None):\n",
    "    fig, ax = plt.subplots(figsize=figsize) \n",
    "    if colormap is None: \n",
    "        colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    dropvals = np.zeros_like(corr)\n",
    "    dropvals[np.triu_indices_from(dropvals)] = True\n",
    "    sns.heatmap(corr, cmap = colormap, linewidths = .5, annot = True, fmt = \".2f\", mask = dropvals, **heatmap_dict)\n",
    "    plt.title(title)\n",
    "    return fig, ax\n",
    "\n",
    "corr_mrwa = train_flattened_mrwa.corr().abs()\n",
    "corr_nzta = train_flattened_nzta.corr().abs()\n",
    "corr_vic = train_flattened_vic.corr().abs()\n",
    "corr_nsw = train_flattened_nsw.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = 0; vmax=1\n",
    "cmap = sns.light_palette('red', as_cmap=True)\n",
    "fig_nzta, ax_nzta = plot_cross_correlation(corr_nzta, f'{\"nzta\".upper()} Correlation', heatmap_dict={'vmin': vmin, 'vmax': vmax}, colormap=cmap)\n",
    "fig_mrwa, ax_mrwa = plot_cross_correlation(corr_mrwa, f'{\"mrwa\".upper()} Correlation', heatmap_dict={'vmin': vmin, 'vmax': vmax}, colormap=cmap)\n",
    "fig_vic, ax_vic = plot_cross_correlation(corr_vic, f'{\"vic\".upper()} Correlation', figsize=(22, 8), heatmap_dict={'vmin': vmin, 'vmax': vmax}, colormap=cmap)\n",
    "fig_nsw, ax_nsw = plot_cross_correlation(corr_nsw, f'{\"nsw\".upper()} Correlation', heatmap_dict={'vmin': vmin, 'vmax': vmax}, colormap=cmap)\n",
    "\n",
    "fig_nzta.tight_layout()\n",
    "fig_nzta.savefig(save_fig_dir / 'nzta_corr.jpeg')\n",
    "fig_mrwa.tight_layout()\n",
    "fig_mrwa.savefig(save_fig_dir / 'mrwa_corr.jpeg')\n",
    "fig_vic.tight_layout()\n",
    "fig_vic.savefig(save_fig_dir / 'vic_corr.jpeg')\n",
    "fig_nsw.tight_layout()\n",
    "fig_nsw.savefig(save_fig_dir / 'nsw_corr.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result grouped by treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.visualization.plot_metric as plot_metric\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_metric_over_time_all_datasets(\n",
    "        result_dict: dict, \n",
    "    ):\n",
    "    \"\"\"plot total accuracy for each of type-treatment pair\"\"\"\n",
    "    uniq_juris= sorted(result_dict.keys(), key=lambda x: juri_order[x])\n",
    "    metric_names = ['Precision', 'Recall', 'F-Score']\n",
    "\n",
    "    treatments_time_dict : Dict[str, set] = {} \n",
    "    for juri in uniq_juris:\n",
    "        prediction_columns : pd.MultiIndex = result_dict[juri]['prediction_columns']\n",
    "        for treatment in set(prediction_columns.get_level_values(1)):\n",
    "            times = set(prediction_columns[prediction_columns.get_level_values(1) == treatment].get_level_values(0))\n",
    "            if 'Treatment between 10 to 30 years' in times:\n",
    "                times.remove('Treatment between 10 to 30 years')\n",
    "            if treatment in treatments_time_dict:\n",
    "                treatments_time_dict[treatment].update(times)\n",
    "            else:\n",
    "                treatments_time_dict[treatment] = times\n",
    "\n",
    "    for treatment, times in treatments_time_dict.items():\n",
    "        fig, axs = plt.subplots(nrows=len(metric_names), ncols=1, figsize=(12, 4 * len(metric_names)))\n",
    "        axs = axs.ravel()\n",
    "        x = np.arange(len(times))\n",
    "        width = 0.1\n",
    "        uniq_types = sorted(times, key=treatment_time_order.__getitem__)\n",
    "\n",
    "        metric_dict = {}\n",
    "        dummy_metric_dict = {}\n",
    "        for juri in uniq_juris:\n",
    "            prediction_columns : pd.MultiIndex = result_dict[juri]['prediction_columns']\n",
    "\n",
    "            # initialise metric dictionaries\n",
    "            metric_dict[juri] = {}\n",
    "            dummy_metric_dict[juri] = {}\n",
    "            for metric in metric_names:\n",
    "                metric_time_dict = OrderedDict()\n",
    "                dummy_metric_time_dict = OrderedDict()\n",
    "                for time_horizon in uniq_types:\n",
    "                    metric_time_dict[time_horizon] = [np.nan, np.nan]\n",
    "                    dummy_metric_time_dict[time_horizon] = np.nan\n",
    "                metric_dict[juri][metric] = metric_time_dict\n",
    "                dummy_metric_dict[juri][metric] = dummy_metric_time_dict\n",
    "                \n",
    "            for i, (time_type, inner_treatment) in enumerate(prediction_columns):\n",
    "                # fill out values for metric dictionary\n",
    "                if inner_treatment != treatment or time_type == 'Treatment between 10 to 30 years':\n",
    "                    continue\n",
    "                running_conf_mat : np.ndarray = np.array(result_dict[juri]['conf_mat'])[:, i, :, :]\n",
    "                metric_dict[juri]['Precision'][time_type] = (running_conf_mat[:, 1, 1] / running_conf_mat[:, :, 1].sum(axis=1))\n",
    "                metric_dict[juri]['Recall'][time_type] = (running_conf_mat[:, 1, 1] / running_conf_mat[:, 1, :].sum(axis=1))\n",
    "                metric_dict[juri]['F-Score'][time_type] = (2 / (1 / metric_dict[juri]['Precision'][time_type] + 1 / metric_dict[juri]['Recall'][time_type]))\n",
    "                # do the same for dummy\n",
    "                for _, conf_mat in result_dict[juri]['conf_mat_dummy'].items():\n",
    "                    running_conf_mat = np.array(conf_mat)[:, i, :, :]\n",
    "                    prec = running_conf_mat[:, 1, 1] / running_conf_mat[:, :, 1].sum(axis=1)\n",
    "                    recall = running_conf_mat[:, 1, 1] / running_conf_mat[:, 1, :].sum(axis=1)\n",
    "                    f_score = 2 / (1 / prec + 1 / recall)\n",
    "                    # get running max over means of multiple dummy strategies\n",
    "                    dummy_metric_dict[juri]['Precision'][time_type] = np.nanmax([np.nanmean(prec), dummy_metric_dict[juri]['Precision'][time_type]])\n",
    "                    dummy_metric_dict[juri]['Recall'][time_type] = np.nanmax([np.nanmean(recall), dummy_metric_dict[juri]['Recall'][time_type]])\n",
    "                    dummy_metric_dict[juri]['F-Score'][time_type] = np.nanmax([np.nanmean(f_score), dummy_metric_dict[juri]['F-Score'][time_type]])\n",
    "\n",
    "        handles = []\n",
    "        acc_bars = []\n",
    "        for i, juri in enumerate(uniq_juris):\n",
    "            # make violin plots\n",
    "            for j, metric_name in enumerate(metric_names):\n",
    "                acc_bars.append(axs[j].violinplot(list(metric_dict[juri][metric_name].values()), positions=x+i*width, widths=width, showmeans=True))\n",
    "            # color violin plots by juri accurately\n",
    "            for violin_plots in acc_bars:\n",
    "                for key, collection in violin_plots.items():\n",
    "                    if key == 'bodies':\n",
    "                        for pc in collection:\n",
    "                            pc.set_facecolor(juri_colors[juri])\n",
    "                            pc.set_alpha(0.3)\n",
    "                            pc.set_edgecolor(juri_colors[juri])\n",
    "                    else:\n",
    "                        collection.set_edgecolor(juri_colors[juri])\n",
    "            handles.append(mpatches.Patch(color=acc_bars[0][\"bodies\"][0].get_facecolor().flatten()))\n",
    "            acc_bars = []\n",
    "            # make chance-level bars\n",
    "            enlist = lambda lst: [[val] for val in lst]\n",
    "            for j, metric_name in enumerate(metric_names):\n",
    "                bp = axs[j].boxplot(enlist(dummy_metric_dict[juri][metric_name].values()), positions=x+i*width, widths=width)\n",
    "                for element in ['boxes', 'whiskers', 'medians', 'caps']:\n",
    "                    plt.setp(bp[element], color='red')\n",
    "\n",
    "        for metric in range(len(metric_names)):\n",
    "            # axs[metric].set_ylabel(metric_names[metric], rotation=0)\n",
    "            axs[metric].set_title(metric_names[metric], loc='left')\n",
    "            axs[metric].set_xticks(x + width * (len(uniq_juris) - 1) / 2)\n",
    "            axs[metric].set_xticklabels(map(year_map_dict.__getitem__, uniq_types))\n",
    "            juri_legend = axs[metric].legend(handles, uniq_juris, bbox_to_anchor=(1, 1), loc=\"upper left\", title='juri')\n",
    "            ##  color top half\n",
    "            curr_xlim = axs[metric].get_xlim()\n",
    "            axs[metric].fill_between(x=[-0.5, x.max() + 1], y1=0.5, y2=1, color=treatment_type_colors[treatment], alpha=0.3, zorder=-1, label='Better than 0.5')\n",
    "            axs[metric].set_xlim(curr_xlim)\n",
    "            # axs[metric].axhline(0.5, xmin=axs[metric].get_xlim()[0], xmax=axs[metric].get_xlim()[1], color='red', linestyle='--', linewidth=0.5, label='0.5 line')\n",
    "            axs[metric].plot([], [], 'r', label='Best chance level')\n",
    "            axs[metric].legend(bbox_to_anchor=(1, 0), loc=\"lower left\", title='Context')\n",
    "            axs[metric].add_artist(juri_legend)\n",
    "            axs[metric].set_ylim((0, 1) if metric_names[metric] != 'accuracy' else (0.8, 1))\n",
    "            axs[metric].grid(True)\n",
    "\n",
    "        if SUFFIX == 'balanced_sampled':\n",
    "            fig.suptitle(f'Performance on validation set of short sections\\n{treatment} - {\" \".join(map(str.capitalize, suffix.split(\"_\")))}', horizontalalignment='center', verticalalignment='top')\n",
    "        else:\n",
    "            fig.suptitle(f'Performance on validation set of short sections - Sampling performed to correct for class imbalance\\n{treatment} - {\" \".join(map(str.capitalize, suffix.split(\"_\")))}', horizontalalignment='center', verticalalignment='top')\n",
    "        plt.tight_layout()\n",
    "        save_path = f'joined_results_{treatment.lower()}.jpg'\n",
    "        plt.savefig(save_fig_dir / save_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_dir = report_dir.parent / 'figures' / 'shared' / ('joined_results_by_treatment' + f'_{SUFFIX}')\n",
    "if save_fig_dir.exists() is False:\n",
    "    save_fig_dir.mkdir()\n",
    "plot_metric_over_time_all_datasets(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result grouped by metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_specific_metric_all_datasets(\n",
    "        result_dict: dict, \n",
    "        rare_class: bool=False,\n",
    "        metric: str='f_score',\n",
    "        anonymised: bool=False\n",
    "    ):\n",
    "    \"\"\"plot total accuracy for each of type-treatment pair\"\"\"\n",
    "    uniq_juris= sorted(result_dict.keys(), key=lambda x: juri_order[x])\n",
    "\n",
    "    treatments_time_dict : Dict[str, set] = {} \n",
    "    for juri in uniq_juris:\n",
    "        prediction_columns : pd.MultiIndex = result_dict[juri]['prediction_columns']\n",
    "        for treatment in set(prediction_columns.get_level_values(1)):\n",
    "            if not rare_class:\n",
    "                if treatment not in ['Resurfacing_SS', 'Resurfacing_AC', 'Rehabilitation']:\n",
    "                    continue\n",
    "            else:\n",
    "                if treatment in ['Resurfacing_SS', 'Resurfacing_AC', 'Rehabilitation']:\n",
    "                    continue\n",
    "            times = set(prediction_columns[prediction_columns.get_level_values(1) == treatment].get_level_values(0))\n",
    "            if 'Treatment between 10 to 30 years' in times:\n",
    "                times.remove('Treatment between 10 to 30 years')\n",
    "            if treatment in treatments_time_dict:\n",
    "                treatments_time_dict[treatment].update(times)\n",
    "            else:\n",
    "                treatments_time_dict[treatment] = times\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\n",
    "    axs = axs.ravel()\n",
    "    for treatment_idx, (treatment, times) in enumerate(sorted(treatments_time_dict.items(), key=lambda tup: treatment_type_order[tup[0]])):\n",
    "        x = np.arange(len(times))\n",
    "        width = 0.1\n",
    "        uniq_types = sorted(times, key=treatment_time_order.__getitem__)\n",
    "\n",
    "        metric_dict = {}\n",
    "        dummy_metric_dict = {}\n",
    "        for juri in uniq_juris:\n",
    "            prediction_columns : pd.MultiIndex = result_dict[juri]['prediction_columns']\n",
    "\n",
    "            metric_dict[juri] = {}\n",
    "            dummy_metric_dict[juri] = {}\n",
    "            metric_time_dict = OrderedDict()\n",
    "            dummy_metric_time_dict = OrderedDict()\n",
    "            for time_horizon in uniq_types:\n",
    "                metric_time_dict[time_horizon] = [np.nan, np.nan]\n",
    "                dummy_metric_time_dict[time_horizon] = np.nan\n",
    "            metric_dict[juri] = metric_time_dict\n",
    "            dummy_metric_dict[juri] = dummy_metric_time_dict\n",
    "            \n",
    "            for i, (time_type, inner_treatment) in enumerate(prediction_columns):\n",
    "                if inner_treatment != treatment or time_type == 'Treatment between 10 to 30 years':\n",
    "                    continue\n",
    "                running_conf_mat : np.ndarray = np.array(result_dict[juri]['conf_mat'])[:, i, :, :]\n",
    "                precision = (running_conf_mat[:, 1, 1] / running_conf_mat[:, :, 1].sum(axis=1))\n",
    "                recall = (running_conf_mat[:, 1, 1] / running_conf_mat[:, 1, :].sum(axis=1))\n",
    "                if metric == 'F-Score':\n",
    "                    metric_dict[juri][time_type] = (2 / (1 / precision + 1 / recall))\n",
    "                elif metric == 'Recall':\n",
    "                    metric_dict[juri][time_type] = recall\n",
    "                elif metric == 'Precision':\n",
    "                    metric_dict[juri][time_type] = precision\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"{metric} not implemented!\")\n",
    "                # compute for dummy\n",
    "                for _, conf_mat in result_dict[juri]['conf_mat_dummy'].items():\n",
    "                    running_conf_mat = np.array(conf_mat)[:, i, :, :]\n",
    "                    precision = running_conf_mat[:, 1, 1] / running_conf_mat[:, :, 1].sum(axis=1)\n",
    "                    recall = running_conf_mat[:, 1, 1] / running_conf_mat[:, 1, :].sum(axis=1)\n",
    "                    f_score = 2 / (1 / precision + 1 / recall)\n",
    "                    if metric == 'F-Score':\n",
    "                        dummy_metric_dict[juri][time_type] = np.nanmax([np.nanmean(f_score), dummy_metric_dict[juri][time_type]])\n",
    "                    elif metric == 'Recall':\n",
    "                        dummy_metric_dict[juri][time_type] = np.nanmax([np.nanmean(recall), dummy_metric_dict[juri][time_type]])\n",
    "                    elif metric == 'Precision':\n",
    "                        dummy_metric_dict[juri][time_type] = np.nanmax([np.nanmean(precision), dummy_metric_dict[juri][time_type]])\n",
    "                    else:\n",
    "                        raise NotImplementedError(f\"{metric} not implemented!\")\n",
    "\n",
    "        handles = []\n",
    "        acc_bars = []\n",
    "        for i, juri in enumerate(uniq_juris):\n",
    "            # make violin plots\n",
    "            acc_bars.append(axs[treatment_idx].violinplot(list(metric_dict[juri].values()), positions=x+i*width, widths=width, showmeans=True))\n",
    "            # color violin plots by juri accurately\n",
    "            for violin_plots in acc_bars:\n",
    "                for key, collection in violin_plots.items():\n",
    "                    if key == 'bodies':\n",
    "                        for pc in collection:\n",
    "                            pc.set_facecolor(juri_colors[juri])\n",
    "                            pc.set_alpha(0.3)\n",
    "                            pc.set_edgecolor(juri_colors[juri])\n",
    "                    else:\n",
    "                        collection.set_edgecolor(juri_colors[juri])\n",
    "            handles.append(mpatches.Patch(color=acc_bars[0][\"bodies\"][0].get_facecolor().flatten()))\n",
    "            acc_bars = []\n",
    "            # make chance-level bars\n",
    "            enlist = lambda lst: [[val] for val in lst]\n",
    "            bp = axs[treatment_idx].boxplot(enlist(dummy_metric_dict[juri].values()), positions=x+i*width, widths=width)\n",
    "            for element in ['boxes', 'whiskers', 'medians', 'caps']:\n",
    "                plt.setp(bp[element], color='red')\n",
    "\n",
    "        if not anonymised:\n",
    "            legend_labels = uniq_juris\n",
    "        else:\n",
    "            legend_labels = ['Jurisdiction {}'.format(num + 1) for num in range(len(uniq_juris))]\n",
    "\n",
    "        axs[treatment_idx].set_ylabel(metric)\n",
    "        axs[treatment_idx].set_title(treatment, loc='left')\n",
    "        axs[treatment_idx].set_xticks(x + width * (len(uniq_juris) - 1) / 2)\n",
    "        axs[treatment_idx].set_xticklabels(map(year_map_dict.__getitem__, uniq_types))\n",
    "        axs[treatment_idx].set_ylim((0, 1))\n",
    "        juri_legend = axs[treatment_idx].legend(handles, legend_labels, bbox_to_anchor=(1, 1), loc=\"upper left\", title='Jurisdiction')\n",
    "        # color top half\n",
    "        curr_xlim = axs[treatment_idx].get_xlim()\n",
    "        axs[treatment_idx].fill_between(x=[-0.5, x.max() + 1], y1=0.5, y2=1, color=treatment_type_colors[treatment], alpha=0.3, zorder=-1, label='Better than 0.5')\n",
    "        axs[treatment_idx].set_xlim(curr_xlim)\n",
    "        axs[treatment_idx].set_ylim((0, 1))\n",
    "        # best chance bars\n",
    "        axs[treatment_idx].plot([], [], 'r', label='Best chance level')\n",
    "        axs[treatment_idx].legend(bbox_to_anchor=(1, 0), loc=\"lower left\", title='Context')\n",
    "        axs[treatment_idx].add_artist(juri_legend)\n",
    "        axs[treatment_idx].grid(True)\n",
    "\n",
    "    if SUFFIX == 'balanced_sampled':\n",
    "        fig.suptitle(f'{metric} performance on validation set of short sections\\nSampling performed to correct for class imbalance', horizontalalignment='center', verticalalignment='top')\n",
    "    else:\n",
    "        fig.suptitle(f'{metric} performance on validation set of short sections', horizontalalignment='center', verticalalignment='top')\n",
    "    plt.tight_layout()\n",
    "    save_path = f'joined_results{\"_rare\" if rare_class else \"\"}_{metric}{\"_anon\" if anonymised else \"\"}.png'\n",
    "    plt.savefig(save_fig_dir / save_path, dpi=200)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig_dir = report_dir.parent / 'figures' / 'shared' / ('joined_results_by_metric' + f'_{SUFFIX}')\n",
    "if save_fig_dir.exists() is False:\n",
    "    save_fig_dir.mkdir(parents=True)\n",
    "\n",
    "for metric in ['Recall', 'Precision', 'F-Score']:\n",
    "    plot_specific_metric_all_datasets(result_dict, rare_class=False, metric=metric, anonymised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redo performance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.visualize import plot_metric_by_treatment_type\n",
    "\n",
    "inner_dir = save_fig_dir / 'redo_valid'\n",
    "if not inner_dir.exists():\n",
    "    inner_dir.mkdir()\n",
    "\n",
    "def plot_metric_each_dataset_redo(\n",
    "        result_dict: dict, \n",
    "    ):\n",
    "    \"\"\"plot total accuracy for each of type-treatment pair\"\"\"\n",
    "    for juri in result_dict.keys():\n",
    "        valid_mat = result_dict[juri]['conf_mat']\n",
    "        prediction_columns = result_dict[juri]['prediction_columns']\n",
    "\n",
    "        plot_metric_by_treatment_type(pd.DataFrame(columns=prediction_columns), valid_mat, \n",
    "            suptitle=f'Performance on validation set of short sections\\n{juri} - {\" \".join(map(str.capitalize, suffix.split(\"_\")))}',\n",
    "            save_path=inner_dir / f'{juri}_valid_redo_{suffix}.jpg'\n",
    "        )\n",
    "\n",
    "plot_metric_each_dataset_redo(result_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
