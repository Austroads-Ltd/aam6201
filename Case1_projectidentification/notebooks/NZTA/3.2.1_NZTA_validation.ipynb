{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.util as util\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from src.visualization.visualize import plot_baseline_metric_by_treatment_type, plot_confusion_matrix_by_treatment_type, plot_metric_by_treatment_type\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from src import DATA_DIR\n",
    "\n",
    "DATASET_NAME = 'NZTA'\n",
    "REPORT_DIR = DATA_DIR.parent / 'reports' / 'figures' / DATASET_NAME\n",
    "if REPORT_DIR.exists() is False:\n",
    "    REPORT_DIR.mkdir(parents=True)\n",
    "\n",
    "experiment_suffix = 'nzta_final_even_split' \n",
    "\n",
    "save_suffix = experiment_suffix\n",
    "load_suffix = experiment_suffix.replace('on_nzta_', '')\n",
    "model_suffix = load_suffix\n",
    "experiment_prefix = 'valid'\n",
    "\n",
    "SAVE_MODEL_DIR = DATA_DIR.parent / 'models' / 'trained' / DATASET_NAME / (load_suffix + '_dir')\n",
    "EXPERIMENT_FOLDER = REPORT_DIR / save_suffix \n",
    "if EXPERIMENT_FOLDER.exists() is False:\n",
    "    EXPERIMENT_FOLDER.mkdir()\n",
    "SAVE_RESULT_DIR = REPORT_DIR.parent.parent / 'raw_results' / DATASET_NAME / (save_suffix + '_dir')\n",
    "if SAVE_RESULT_DIR.exists() is False:\n",
    "    SAVE_RESULT_DIR.mkdir(parents=True)\n",
    "\n",
    "save_path_meta_dict = {\n",
    "    'experiment_prefix': experiment_prefix,\n",
    "    'experiment_suffix': save_suffix,\n",
    "    'experiment_folder': EXPERIMENT_FOLDER,\n",
    "    'dataset_name': DATASET_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_suffix = 'final_no_offset'\n",
    "if 'on_mrwa' in experiment_suffix:\n",
    "    data_suffix = 'mrwa_' + data_suffix\n",
    "    from src.nzta_configs.final_config import CONFIG as NZTA_CONFIG\n",
    "    from src.mrwa_configs.final_config import CONFIG as MRWA_CONFIG\n",
    "\n",
    "    with open(MRWA_CONFIG['preprocessing']['state_save_path'] / (f'preprocessing_state_dict_{data_suffix.replace(\"_no_offset\", \"\")}.sav'), 'rb') as f:\n",
    "        mrwa_saved_state_dict = pickle.load(f)\n",
    "    with open(NZTA_CONFIG['preprocessing']['state_save_path'] / (f'preprocessing_state_dict_{data_suffix.replace(\"mrwa\", \"nzta\").replace(\"_no_offset\", \"\")}.sav'), 'rb') as f:\n",
    "        nzta_saved_state_dict = pickle.load(f)\n",
    "\n",
    "    mrwa_train_feature_data = util.load_data(source=DATA_DIR / 'processed' / 'MRWA' / data_suffix.replace('_no_offset', '') / f'train_flattened_data{\"_\" + data_suffix if data_suffix else \"\"}.csv') \n",
    "    mrwa_train_label_data = util.load_data(source=DATA_DIR / 'processed' / 'MRWA' / data_suffix.replace('_no_offset', '') / f'train_flattened_labels{\"_\" + data_suffix if data_suffix else \"\"}.csv', header=[0, 1]) \n",
    "    nzta_train_feature_data = util.load_data(source=DATA_DIR / 'processed' / 'NZTA' / data_suffix.replace(\"mrwa\", \"nzta\").replace('_no_offset', '') / f'train_flattened_data{\"_\" + data_suffix.replace(\"mrwa\", \"nzta\") if data_suffix else \"\"}.csv') \n",
    "\n",
    "    # rescale all features\n",
    "    for feature in mrwa_train_feature_data.columns:\n",
    "        try:\n",
    "            mrwa_train_feature_data.loc[:, feature] = mrwa_saved_state_dict['scaler']\\\n",
    "                                                    [feature.replace('_df0|idx=0', '')]\\\n",
    "                                                    .inverse_transform(mrwa_train_feature_data.loc[:, feature].values.reshape(-1, 1))\\\n",
    "                                                    .flatten()\n",
    "            mrwa_train_feature_data.loc[:, feature] = nzta_saved_state_dict['scaler']\\\n",
    "                                                    [feature.replace('_df0|idx=0', '')]\\\n",
    "                                                    .transform(mrwa_train_feature_data.loc[:, feature].values.reshape(-1, 1))\\\n",
    "                                                    .flatten()\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    feature_data = mrwa_train_feature_data\n",
    "    label_data = mrwa_train_label_data\n",
    "\n",
    "    feature_data['Pavement Type_Rigid_df0|idx=0'] = 1 \\\n",
    "        - feature_data['Pavement Type_Flexible_df0|idx=0'].replace({\n",
    "            nzta_train_feature_data['Pavement Type_Flexible_df0|idx=0'].min(): 0.0,\n",
    "            nzta_train_feature_data['Pavement Type_Flexible_df0|idx=0'].max(): 1.0,\n",
    "        })\\\n",
    "        - feature_data['Pavement Type_Other_df0|idx=0'].replace({\n",
    "            feature_data['Pavement Type_Other_df0|idx=0'].min(): 0.0,\n",
    "            feature_data['Pavement Type_Other_df0|idx=0'].max(): 1.0,\n",
    "        })\n",
    "\n",
    "    feature_data.loc[:, 'Pavement Type_Rigid_df0|idx=0'] = feature_data['Pavement Type_Rigid_df0|idx=0'].replace({\n",
    "        0: nzta_train_feature_data['Pavement Type_Rigid_df0|idx=0'].min(),\n",
    "        1: nzta_train_feature_data['Pavement Type_Rigid_df0|idx=0'].max(),\n",
    "    })\n",
    "    feature_data = feature_data[nzta_train_feature_data.columns]\n",
    "elif 'even_split' in experiment_suffix:\n",
    "    data_suffix = 'nzta_final_even_split'\n",
    "    feature_data = util.load_data(source=DATA_DIR / 'processed' / 'NZTA' / data_suffix / f'valid_flattened_data{\"_\" + data_suffix if data_suffix else \"\"}.csv') \n",
    "    label_data = util.load_data(source=DATA_DIR / 'processed' / 'NZTA' / data_suffix / f'valid_flattened_labels{\"_\" + data_suffix if data_suffix else \"\"}.csv', header=[0, 1]) \n",
    "else:\n",
    "    data_suffix = 'nzta_' + data_suffix\n",
    "    feature_data = util.load_data(source=DATA_DIR / 'processed' / 'NZTA' / data_suffix.replace('_no_offset', '') / f'valid_flattened_data{\"_\" + data_suffix if data_suffix else \"\"}.csv') \n",
    "    label_data = util.load_data(source=DATA_DIR / 'processed' / 'NZTA' / data_suffix.replace('_no_offset', '') / f'valid_flattened_labels{\"_\" + data_suffix if data_suffix else \"\"}.csv', header=[0, 1]) \n",
    "\n",
    "DATASET_NAME = 'NZTA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "def update_summary_dict(current: dict, y_pred: np.ndarray, y_true: pd.Series, probs: np.ndarray, multiclass_roc: str='raise', labels: list=None) -> None:\n",
    "    \"\"\"\n",
    "    Given a current dict of summary statistics, append the new statistics computed from the new predictions \n",
    "    \n",
    "    Args:\n",
    "        x: input\n",
    "        y: true labels\n",
    "        model: the machine learning model\n",
    "    \"\"\"\n",
    "    summary_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0, labels=labels) # classification statistics for each label and each type of average weighting\n",
    "    auc = roc_auc_score(y_true, probs, average=None, multi_class=multiclass_roc)\n",
    "\n",
    "    # dict of summary statistics and feature importance\n",
    "    if y_true.nunique() > 2:\n",
    "        weighted_dict = summary_dict['weighted avg'] # classification reports for weighted average\n",
    "    elif y_true.nunique() == 2:\n",
    "        # if target is binary, we care only about precision, recall, and f1-score for the minority class\n",
    "        weighted_dict = summary_dict[labels[1] if labels is not None else str(y_true.value_counts().sort_values(ascending=True).index[0])]\n",
    "\n",
    "    current['f1-score'].append(weighted_dict['f1-score'])\n",
    "    current['precision'].append(weighted_dict['precision'])\n",
    "    current['recall'].append(weighted_dict['recall'])\n",
    "    current['accuracy'].append(summary_dict['accuracy'])\n",
    "    current['auc'].append(auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(estimator_type: str):\n",
    "    with open(SAVE_MODEL_DIR / f'train_{estimator_type}_timehorizon{\"_\" + model_suffix if model_suffix else \"\"}.pkl', 'rb') as f:\n",
    "        time_horizon_models = pickle.load(f)\n",
    "    return time_horizon_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(estimator_type: str, feature_data: pd.DataFrame, project_label: pd.DataFrame, dropped_columns: pd.Index, prediction_columns: pd.Index, pad: bool=True):\n",
    "    running_conf_matrix = []\n",
    "    summary_dict = {key: [] for key in ['f1-score', 'accuracy', 'auc', 'precision', 'recall']}\n",
    "    models = load_model(estimator_type)\n",
    "\n",
    "    if pad is False:\n",
    "        # the model can predict more than what we want to evaluate on\n",
    "        # so we locate only the desired columns in project label\n",
    "        inner = prediction_columns.intersection(project_label.columns, sort=False) \n",
    "        assert len(project_label.columns) == len(inner) # labels passed in must all be available\n",
    "        project_label = project_label[inner] # ensure order is correct\n",
    "    else:\n",
    "        # check all labels to be evaluated is in prediction columns and padded dropped columns\n",
    "        assert set(project_label.columns) - set(prediction_columns.append(dropped_columns)) == set()\n",
    "        assert len(prediction_columns.intersection(dropped_columns)) == 0\n",
    "        inner = project_label.columns\n",
    "\n",
    "    start_t = time.time()\n",
    "    for model in models: \n",
    "        preds = model.predict(feature_data)\n",
    "        # pad preds with columns we dropped\n",
    "        if pad:\n",
    "            preds = np.hstack((preds, np.zeros((preds.shape[0], len(dropped_columns)))))\n",
    "            preds = pd.DataFrame(preds, columns=prediction_columns.append(dropped_columns))[inner]\n",
    "        else:\n",
    "            preds = pd.DataFrame(preds, columns=prediction_columns)[inner]\n",
    "        # confusion matrix\n",
    "        if project_label.shape[1] > 1:\n",
    "            conf_matrix = multilabel_confusion_matrix(project_label, preds)\n",
    "        else:\n",
    "            probs = model.predict_proba(feature_data)\n",
    "            update_summary_dict(summary_dict, preds, project_label.iloc[:, 0], probs[:, 1])\n",
    "            conf_matrix = confusion_matrix(project_label, preds)\n",
    "        running_conf_matrix.append(conf_matrix)\n",
    "\n",
    "    with open(SAVE_RESULT_DIR / f'valid_{estimator_type}_rawconfmat_{save_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(running_conf_matrix, f)\n",
    "\n",
    "    print(\"Evaluation completed. Time taken: \", time.time() - start_t)\n",
    "    return running_conf_matrix, summary_dict\n",
    "\n",
    "def evaluate_baseline(feature_data: pd.DataFrame, project_label: pd.DataFrame, dropped_columns: pd.Index, prediction_columns: pd.Index, pad: bool=True):\n",
    "    models_by_strat = load_model('dummy')\n",
    "    running_conf_matrix = {strat: [] for strat in models_by_strat.keys()}\n",
    "    \n",
    "    if pad is False:\n",
    "        # the model can predict more than what we want to evaluate on\n",
    "        # so we locate only the desired columns in project label\n",
    "        inner = prediction_columns.intersection(project_label.columns, sort=False) \n",
    "        assert len(project_label.columns) == len(inner) # labels passed in must all be available\n",
    "        project_label = project_label[inner] # ensure order is correct\n",
    "    else:\n",
    "        # check all labels to be evaluated is in prediction columns and padded dropped columns\n",
    "        assert set(project_label.columns) - set(prediction_columns.append(dropped_columns)) == set()\n",
    "        assert len(prediction_columns.intersection(dropped_columns)) == 0\n",
    "        inner = project_label.columns\n",
    "\n",
    "    start_t = time.time()\n",
    "    for strat, models in models_by_strat.items(): \n",
    "        for model in models:\n",
    "            preds = model.predict(feature_data)\n",
    "            # pad preds with columns we dropped\n",
    "            if pad:\n",
    "                preds = np.hstack((preds, np.zeros((preds.shape[0], len(dropped_columns)))))\n",
    "                preds = pd.DataFrame(preds, columns=prediction_columns.append(dropped_columns))[inner]\n",
    "            else:\n",
    "                preds = pd.DataFrame(preds, columns=prediction_columns)[inner]\n",
    "            # confusion matrix\n",
    "            conf_matrix = multilabel_confusion_matrix(project_label, preds)\n",
    "            running_conf_matrix[strat].append(conf_matrix)\n",
    "    \n",
    "    with open(SAVE_RESULT_DIR / f'valid_dummy_rawconfmat_{save_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(running_conf_matrix, f)\n",
    "\n",
    "    print(\"Evaluation completed. Time taken: \", time.time() - start_t)\n",
    "    return running_conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on time horizon\n",
    "\n",
    "Given the above's result, we can train the model chiefly on projects with at least a treatment, since it is trivial to learn to otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_str(df):\n",
    "    return df.astype(str).agg(''.join, axis=1)\n",
    "\n",
    "project_label_valid = label_data.drop(columns=['no_project_flag'], level=0)\n",
    "# project_label_valid = pd.DataFrame(label_data.iloc[:, 0].rename('no_project_flag'))\n",
    "with open(SAVE_MODEL_DIR / f'train_labels_columns{\"_\" + load_suffix if load_suffix else \"\"}.pkl', 'rb') as f:\n",
    "    prediction_columns = pickle.load(f)\n",
    "\n",
    "dropped_columns = project_label_valid.columns.difference(prediction_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if PAD is true, evaluate label not seen in training with most common class\n",
    "PAD = False \n",
    "if not PAD:\n",
    "    # for transfer\n",
    "    inner = prediction_columns.intersection(project_label_valid.columns, sort=False) \n",
    "    project_label_valid = project_label_valid[prediction_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_running_conf_matrix, lr_summary_dict = evaluate('LR', feature_data, project_label_valid, dropped_columns, prediction_columns, pad=PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_treatment_type(project_label_valid, lr_running_conf_matrix, estimator_type='LR', **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_by_treatment_type(project_label_valid, lr_running_conf_matrix, estimator_type='LR', per_row=5, figsize=(20, 15), **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_running_conf_matrix, xgb_summary_dict = evaluate('XGB', feature_data, project_label_valid, dropped_columns, prediction_columns, pad=PAD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_by_treatment_type(project_label_valid, xgb_running_conf_matrix, estimator_type='XGB', **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_by_treatment_type(project_label_valid, xgb_running_conf_matrix, estimator_type='XGB', per_row=5, figsize=(20, 15), **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_running_conf_matrix = evaluate_baseline(feature_data, project_label_valid, dropped_columns, prediction_columns, pad=PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_baseline_metric_by_treatment_type(project_label_valid, baseline_running_conf_matrix, estimator_type='Dummy', **save_path_meta_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
