{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import src.data.resampling as resampling\n",
    "import src.util as util\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from src.nzta_configs.final_config import CONFIG \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from src.visualization.visualize import plot_baseline_metric_by_treatment_type, plot_metric_by_treatment_type, plot_confusion_matrix_by_treatment_type\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from data import DATA_DIR\n",
    "\n",
    "DATASET_NAME = 'NZTA'\n",
    "REPORT_DIR = DATA_DIR.parent / 'reports' / 'figures' / DATASET_NAME\n",
    "if REPORT_DIR.exists() is False:\n",
    "    REPORT_DIR.mkdir(parents=True)\n",
    "\n",
    "data_suffix = 'nzta_final_no_offset'\n",
    "\n",
    "train_feature_data = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / f'{data_suffix.replace(\"_no_offset\", \"\")}' / f'train_flattened_data{\"_\" + data_suffix if data_suffix else \"\"}.csv') \n",
    "train_label_data = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / f'{data_suffix.replace(\"_no_offset\", \"\")}' / f'train_flattened_labels{\"_\" + data_suffix if data_suffix else \"\"}.csv', header=[0, 1]) \n",
    "train_index_data = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / f'{data_suffix.replace(\"_no_offset\", \"\")}' / f'train_flattened_index{\"_\" + data_suffix.replace(\"_no_offset\", \"\") if data_suffix else \"\"}.csv') \n",
    "valid_feature_data = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / f'{data_suffix.replace(\"_no_offset\", \"\")}' / f'valid_flattened_data{\"_\" + data_suffix if data_suffix else \"\"}.csv') \n",
    "valid_label_data = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME /  f'{data_suffix.replace(\"_no_offset\", \"\")}' / f'valid_flattened_labels{\"_\" + data_suffix if data_suffix else \"\"}.csv', header=[0, 1]) \n",
    "valid_index_data = util.load_data(source=DATA_DIR / 'processed' / DATASET_NAME / f'{data_suffix.replace(\"_no_offset\", \"\")}' / f'valid_flattened_index{\"_\" + data_suffix.replace(\"_no_offset\", \"\") if data_suffix else \"\"}.csv') \n",
    "\n",
    "experiment_suffix = 'nzta_final_even_split'\n",
    "experiment_prefix = 'train'\n",
    "\n",
    "EXPERIMENT_FOLDER = REPORT_DIR / experiment_suffix\n",
    "SAVE_MODEL_DIR = DATA_DIR.parent / 'models' / 'trained' / DATASET_NAME / (experiment_suffix + '_dir')\n",
    "SAVE_RESULT_DIR = REPORT_DIR.parent.parent / 'raw_results' / DATASET_NAME / (experiment_suffix + '_dir')\n",
    "\n",
    "if SAVE_RESULT_DIR.exists() is False:\n",
    "    SAVE_RESULT_DIR.mkdir(parents=True)\n",
    "if SAVE_MODEL_DIR.exists() is False:\n",
    "    SAVE_MODEL_DIR.mkdir(parents=True)\n",
    "if EXPERIMENT_FOLDER.exists() is False:\n",
    "    EXPERIMENT_FOLDER.mkdir()\n",
    "\n",
    "save_path_meta_dict = {\n",
    "    'experiment_prefix': experiment_prefix,\n",
    "    'experiment_suffix': experiment_suffix,\n",
    "    'experiment_folder': EXPERIMENT_FOLDER,\n",
    "    'dataset_name': DATASET_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resplit train-valid according to roadid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'even_split' in experiment_suffix:\n",
    "    # merge everything\n",
    "    feature_data = pd.concat([train_feature_data, valid_feature_data], axis=0)\n",
    "    label_data = pd.concat([train_label_data, valid_label_data], axis=0)\n",
    "    index_data = pd.concat([train_index_data, valid_index_data], axis=0)\n",
    "    assert np.all(feature_data.index == label_data.index)\n",
    "    feature_data = feature_data.reset_index(drop=True)\n",
    "    label_data = label_data.reset_index(drop=True)\n",
    "    index_data = index_data.reset_index(drop=True)\n",
    "\n",
    "    # split half, such that valid and train each makes up half a road and a section belonging to the train immediately follows a valid section\n",
    "    sorted_index = index_data.sort_values(by=['RoadID', 'Start']).index\n",
    "    train_feature_data = feature_data.loc[[idx for i, idx in enumerate(sorted_index) if i % 2 == 0]]\n",
    "    train_label_data = label_data.loc[[idx for i, idx in enumerate(sorted_index) if i % 2 == 0]]\n",
    "    train_index_data = index_data.loc[[idx for i, idx in enumerate(sorted_index) if i % 2 == 0]]\n",
    "\n",
    "    valid_feature_data = feature_data.loc[[idx for i, idx in enumerate(sorted_index) if i % 2 == 1]]\n",
    "    valid_label_data = label_data.loc[[idx for i, idx in enumerate(sorted_index) if i % 2 == 1]]\n",
    "    valid_index_data = index_data.loc[[idx for i, idx in enumerate(sorted_index) if i % 2 == 1]]\n",
    "\n",
    "    # save new data for retrieval\n",
    "    save_data_dir = DATA_DIR / 'processed' / DATASET_NAME / experiment_suffix.replace('_no_offset', '')\n",
    "    if save_data_dir.exists() is False:\n",
    "        save_data_dir.mkdir()\n",
    "\n",
    "    train_feature_data.to_csv(save_data_dir / f'train_flattened_data_{experiment_suffix}.csv', index=False)\n",
    "    train_label_data.to_csv(save_data_dir / f'train_flattened_labels_{experiment_suffix}.csv', index=False)\n",
    "    train_index_data.to_csv(save_data_dir / f'train_flattened_index_{experiment_suffix}.csv', index=False)\n",
    "\n",
    "    valid_feature_data.to_csv(save_data_dir / f'valid_flattened_data_{experiment_suffix}.csv', index=False)\n",
    "    valid_label_data.to_csv(save_data_dir / f'valid_flattened_labels_{experiment_suffix}.csv', index=False)\n",
    "    valid_index_data.to_csv(save_data_dir / f'valid_flattened_index_{experiment_suffix}.csv', index=False)\n",
    "\n",
    "    # set config to do not train-test split\n",
    "    CONFIG['sampling']['test_size'] = 0\n",
    "\n",
    "# set feature data, label data to train set\n",
    "feature_data = train_feature_data\n",
    "label_data = train_label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.models_util import make_constraint\n",
    "from typing import List\n",
    "\n",
    "def get_model(estimator_type: str, use_constraint: bool=False, treatment: str=None, feature_names: List[str]=None, **kwargs):\n",
    "    if estimator_type == 'LR':\n",
    "        return LogisticRegression(max_iter=1000)\n",
    "    elif estimator_type == 'XGB':\n",
    "        if use_constraint:\n",
    "            if treatment is None or feature_names is None: raise ValueError(\"Treatment and feature names must be provided if constraint is applied!\") \n",
    "            model_constraints = make_constraint(treatment, feature_names)\n",
    "        else:\n",
    "            model_constraints = None\n",
    "        return XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss', monotone_constraints=model_constraints, **kwargs)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def get_coeff(estimator):\n",
    "    if isinstance(estimator, LogisticRegression):\n",
    "        return estimator.coef_[0]\n",
    "    elif isinstance(estimator, XGBClassifier):\n",
    "        bst = estimator.get_booster()\n",
    "        importance_dicts = []\n",
    "        for importance_type in ['weight', 'gain', 'cover', 'total_gain', 'total_cover']:\n",
    "            importance_dicts.append(bst.get_score(importance_type=importance_type))\n",
    "        return importance_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multioutput_classifier_with_resampling(estimator_type: str, feature_data, project_label, config, model_params: dict={}):\n",
    "    running_coeffs = [[] for _ in range(len(project_label.columns))]\n",
    "    running_conf_matrix = []\n",
    "    train_running_conf_matrix = []\n",
    "    dummy_running_conf_matrix = {strat: [] for strat in ['stratified', 'most_frequent', 'uniform']}\n",
    "    models = []\n",
    "    dummy_models = {strat: [] for strat in ['stratified', 'most_frequent', 'uniform']}\n",
    "\n",
    "    # training\n",
    "    start_t = time.time()\n",
    "    for i, (x_train, x_test, y_train, y_test) in enumerate(resampling.resample_with_split(feature_data, project_label, config)):\n",
    "\n",
    "        assert y_test.columns.equals(project_label.columns)\n",
    "        assert y_train.columns.equals(project_label.columns)\n",
    "        model = MultiOutputClassifier(get_model(estimator_type, **model_params), n_jobs=6) # ovr for binary data, multinomial for multi-class problem\n",
    "        model.fit(x_train, y_train)\n",
    "        models.append(model)\n",
    "\n",
    "        preds = model.predict(x_test)\n",
    "        train_preds = model.predict(x_train)\n",
    "\n",
    "        # running importance coefficients\n",
    "        for i in range(len(project_label.columns)):\n",
    "            running_coeffs[i].append(get_coeff(model)) # in order of the inputed features: feature_data.columns\n",
    "\n",
    "        # confusion matrix\n",
    "        conf_matrix = multilabel_confusion_matrix(y_test, preds)\n",
    "        running_conf_matrix.append(conf_matrix)\n",
    "        train_conf_matrix = multilabel_confusion_matrix(y_train, train_preds)\n",
    "        train_running_conf_matrix.append(train_conf_matrix)\n",
    "\n",
    "        # train and test dummy model\n",
    "        for strategy in dummy_running_conf_matrix.keys():\n",
    "            dummy = MultiOutputClassifier(DummyClassifier(strategy=strategy))\n",
    "            dummy.fit(x_train, y_train)\n",
    "            dummy_conf_mat = multilabel_confusion_matrix(y_test, dummy.predict(x_test))\n",
    "            dummy_running_conf_matrix[strategy].append(dummy_conf_mat)\n",
    "            dummy_models[strategy].append(dummy)\n",
    "\n",
    "    # turn list of ndarrays into a numpy array\n",
    "    running_conf_matrix = np.array(running_conf_matrix)\n",
    "    train_running_conf_matrix = np.array(train_running_conf_matrix)\n",
    "    for strat in dummy_running_conf_matrix.keys():\n",
    "        dummy_running_conf_matrix[strat] = np.array(dummy_running_conf_matrix[strat])\n",
    "\n",
    "    print(\"Training completed. Time taken: \", time.time() - start_t)\n",
    "    print(\"Saving a sample of trained models...\")\n",
    "    saved_models = np.random.choice(models, size=min(10, len(models)), replace=False)\n",
    "    with open(SAVE_MODEL_DIR / f'train_{estimator_type}_timehorizon_{experiment_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(saved_models, f)\n",
    "    with open(SAVE_RESULT_DIR / f'train_{estimator_type}_rawconfmat_{experiment_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(running_conf_matrix, f)\n",
    "    with open(SAVE_RESULT_DIR / f'train_{estimator_type}_rawconfmat_trainset_{experiment_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(train_running_conf_matrix, f)\n",
    "    save_dummys = {strat: np.random.choice(models, size=min(10, len(models)), replace=False) for strat, models in dummy_models.items()}\n",
    "    with open(SAVE_MODEL_DIR / f'train_dummy_timehorizon_{experiment_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(save_dummys, f)\n",
    "\n",
    "    return running_conf_matrix, running_coeffs, train_running_conf_matrix, dummy_running_conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on time horizon\n",
    "\n",
    "Given the above's result, we can train the model chiefly on projects with at least a treatment, since it is trivial to learn to otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_str(df):\n",
    "    return df.astype(str).agg(''.join, axis=1)\n",
    "\n",
    "project_label = label_data.drop(columns=['no_project_flag'], level=0) # this can be inferred by the previous columns\n",
    "project_label = project_label.sort_index(level=1, axis=1)\n",
    "\n",
    "has_project_train = feature_data\n",
    "\n",
    "while True:\n",
    "    # drop treatment/time pair where they do not have at least 2 classes\n",
    "    key_count = project_label.sum(axis=0)\n",
    "    project_label = project_label.drop(columns=key_count[key_count == 0].index)\n",
    "\n",
    "    # get hash of each sample as string of the flags for each treatment/time pair\n",
    "    target = as_str(project_label)\n",
    "    target_count = target.value_counts()\n",
    "\n",
    "    # drop samples where their unique class (i.e. combination of treatment/time pair) has only 1 value or less\n",
    "    project_label = project_label[target.isin(target_count[target_count > 100].index)]\n",
    "    has_project_train = has_project_train[target.isin(target_count[target_count > 100].index)]\n",
    "\n",
    "    if (project_label.sum(axis=0) != 0).all():\n",
    "        break\n",
    "\n",
    "with open(SAVE_MODEL_DIR / f'train_labels_columns{\"_\" + experiment_suffix if experiment_suffix else \"\"}.pkl', 'wb') as f:\n",
    "    pickle.dump(project_label.columns, f)\n",
    "CONFIG['sampling']['method_params']['index_row'] = as_str # set method for identifying type of each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.sampling.kfold = 5\n",
    "CONFIG.sampling.n_sample_per_fold = 1\n",
    "CONFIG.random_seed = 200\n",
    "CONFIG.sampling.method = 'balanced' if 'balanced' in experiment_suffix else 'none'\n",
    "if 'even_split' in experiment_suffix:\n",
    "    CONFIG.sampling.test_size = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_running_conf_matrix, lr_running_coeffs, lr_train_running_conf_matrix, lr_dummy_running_conf_matrix = \\\n",
    "    train_multioutput_classifier_with_resampling('LR', has_project_train, project_label, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot total accuracy for each of type-treatment pair\n",
    "save_path_meta_dict['experiment_prefix'] = 'Testset'\n",
    "plot_metric_by_treatment_type(project_label, lr_running_conf_matrix, estimator_type='LR', **save_path_meta_dict)\n",
    "save_path_meta_dict['experiment_prefix'] = 'Trainset'\n",
    "plot_metric_by_treatment_type(project_label, lr_train_running_conf_matrix, estimator_type='LR', **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_meta_dict['experiment_prefix'] = 'Testset'\n",
    "plot_confusion_matrix_by_treatment_type(project_label, lr_running_conf_matrix, estimator_type='LR', per_row=3, figsize=(18, 16), **save_path_meta_dict)\n",
    "save_path_meta_dict['experiment_prefix'] = 'Trainset'\n",
    "plot_confusion_matrix_by_treatment_type(project_label, lr_train_running_conf_matrix, estimator_type='LR', per_row=3, figsize=(18, 16), **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.sampling.n_sample_per_fold = 1\n",
    "CONFIG.sampling.kfold = 5\n",
    "CONFIG.random_seed = 100\n",
    "CONFIG.sampling.method = 'balanced' if ('balanced' in experiment_suffix) else 'none'\n",
    "if 'even_split' in experiment_suffix:\n",
    "    CONFIG.sampling.test_size = 0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_running_conf_matrix, xgb_running_coeffs, xgb_train_running_conf_matrix, xgb_dummy_running_conf_matrix =\\\n",
    "    train_multioutput_classifier_with_resampling('XGB', has_project_train, project_label, CONFIG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_meta_dict['experiment_prefix'] = 'Testset'\n",
    "plot_metric_by_treatment_type(project_label, xgb_running_conf_matrix, estimator_type='XGB', **save_path_meta_dict)\n",
    "save_path_meta_dict['experiment_prefix'] = 'Trainset'\n",
    "plot_metric_by_treatment_type(project_label, xgb_train_running_conf_matrix, estimator_type='XGB', **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive methods results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_meta_dict['experiment_prefix'] = 'Testset'\n",
    "plot_baseline_metric_by_treatment_type(project_label, xgb_dummy_running_conf_matrix, estimator_type='XGB', **save_path_meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_meta_dict['experiment_prefix'] = 'Testset'\n",
    "plot_confusion_matrix_by_treatment_type(project_label, xgb_running_conf_matrix, estimator_type='XGB', per_row=4, figsize=(16, 10), **save_path_meta_dict)\n",
    "save_path_meta_dict['experiment_prefix'] = 'Trainset'\n",
    "plot_confusion_matrix_by_treatment_type(project_label, xgb_train_running_conf_matrix, estimator_type='XGB', per_row=4, figsize=(16, 10), **save_path_meta_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
