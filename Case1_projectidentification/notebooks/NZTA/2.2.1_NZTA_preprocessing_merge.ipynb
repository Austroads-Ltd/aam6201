{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import src.util as util\n",
    "import src.features.preprocessing as preprocessing \n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook preprocesses a dataset through the following steps in order:\n",
    "\n",
    "- Apply transformations \n",
    "- Adding new columns\n",
    "- Filtering the dataframe\n",
    "- Dropping unused columns\n",
    "\n",
    "As detailed in the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from src.nzta_configs.final_config import DATA_DIR\n",
    "NZTA_DATADIR = Path(DATA_DIR) / 'interim' / 'NZTA'\n",
    "\n",
    "DATASET = 'NZTA'\n",
    "experiment_suffix = f'nzta_final'\n",
    "\n",
    "rutting_skid = pd.read_csv(NZTA_DATADIR / '100m_Rutting_Skid_new.csv')\n",
    "structure = pd.read_csv(NZTA_DATADIR / '100m_Surface_Structure_new.csv')\n",
    "carriageway = pd.read_csv(NZTA_DATADIR / '100m_Carriageway_new.csv')\n",
    "pavement_layer = pd.read_csv(NZTA_DATADIR / '100m_Pavement_Layer_new.csv')\n",
    "traffic = pd.read_csv(NZTA_DATADIR / '100m_Traffic_new.csv')\n",
    "deflection_crack = pd.read_csv(NZTA_DATADIR / '100m_Deflection_Cracking_new.csv')\n",
    "\n",
    "cleaned_rutting = rutting_skid.rename(columns={\n",
    "    'Road Name': 'RoadID',\n",
    "    'Year': 'Date of condition data'\n",
    "})\n",
    "cleaned_rutting.loc[:, 'Date of condition data'] = cleaned_rutting['Date of condition data'].astype(np.datetime64)\n",
    "\n",
    "cleaned_carriageway = carriageway.rename(columns={\n",
    "    'Road': 'RoadID'\n",
    "})\n",
    "\n",
    "cleaned_structure = structure.rename(columns={\n",
    "    'Road ID': 'RoadID',\n",
    "    'Age': 'Surface age',\n",
    "})\n",
    "cleaned_structure.loc[:, 'Surfacing Date'] = cleaned_structure['Surfacing Date'].astype(np.datetime64)\n",
    "cleaned_structure = cleaned_structure[cleaned_structure['Surfacing Date'] < pd.to_datetime('2020', format='%Y')]\n",
    "\n",
    "cleaned_pavement = pavement_layer.rename(columns={\n",
    "    'Road': 'RoadID',\n",
    "    'Age': 'Pavement age',\n",
    "})\n",
    "\n",
    "cleaned_def_crack = deflection_crack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change traffic to make them yearless\n",
    "traffic.loc[:, 'Count Date'] = traffic['Count Date'].astype(np.datetime64)\n",
    "cleaned_traffic = traffic.sort_values(by='Count Date')\n",
    "old_shape = cleaned_traffic.shape\n",
    "cleaned_traffic = cleaned_traffic.drop_duplicates(subset=['Road', 'Start'], keep='first')\n",
    "cleaned_traffic = cleaned_traffic.sort_values(by=['Road', 'Start']).drop(columns=['Count Date'])\n",
    "\n",
    "cleaned_traffic = cleaned_traffic.rename(columns={\n",
    "    'Road': 'RoadID'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dfs = [cleaned_rutting, cleaned_structure, cleaned_pavement, cleaned_carriageway, cleaned_traffic, cleaned_def_crack]\n",
    "cleaned_yearless_dfs = [cleaned_pavement, cleaned_carriageway, cleaned_traffic, cleaned_def_crack]\n",
    "cleaned_year_dfs = [cleaned_rutting, cleaned_structure]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge yearless dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roads = set.union(*(set(df['RoadID']) for df in cleaned_yearless_dfs))\n",
    "shared_roads = set.intersection(*(set(df['RoadID']) for df in cleaned_yearless_dfs))\n",
    "all_indices = set.union(*(set(df[['RoadID', 'Start']].itertuples(index=False, name=None)) for df in cleaned_yearless_dfs))\n",
    "shared_indices = set.intersection(*(set(df[['RoadID', 'Start']].itertuples(index=False, name=None)) for df in cleaned_yearless_dfs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce, partial\n",
    "\n",
    "index = ['RoadID', 'Start']\n",
    "indexed_yearless_dfs = [df.dropna(subset=index, how='any').set_index(index) for df in cleaned_yearless_dfs]\n",
    "cleaned_yearless = reduce(partial(pd.merge, how='outer', left_index=True, right_index=True, validate='1:1'), indexed_yearless_dfs)\n",
    "cleaned_yearless = cleaned_yearless.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rutting_structure = cleaned_rutting.merge(cleaned_structure, \n",
    "    how='outer', \n",
    "    on=['RoadID', 'Start'], \n",
    "    validate='m:m',\n",
    "    suffixes=['_traffic', '_structure']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rutting_structure = merged_rutting_structure[\n",
    "    merged_rutting_structure['Date of condition data'] > \\\n",
    "        merged_rutting_structure['Surfacing Date']\n",
    "]\n",
    "\n",
    "cleaned_rutting_structure = \\\n",
    "    cleaned_rutting_structure.sort_values(by='Surfacing Date', ascending=False)\\\n",
    "                             .drop_duplicates(subset=['RoadID', 'Start', 'Date of condition data'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merge_all = cleaned_rutting_structure.merge(cleaned_yearless, on=['RoadID', 'Start'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nzta_configs.final_config import CONFIG, FeatureAdder\n",
    "from IPython.display import display\n",
    "cleaned_added_df = FeatureAdder()(cleaned_merge_all)\n",
    "display(cleaned_added_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_added_df.to_csv(NZTA_DATADIR / '100m_Merged_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising, Imputation, and Encoding Boilerplate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncodeImputeNormalizeContainer:\n",
    "    \"\"\"\n",
    "    Container for encoding, imputation, and normalization operations, in that order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_encoding = None # remember feature encoding for future\n",
    "        self.date_encoding = None # remember date encoding\n",
    "        self.feature_scaling = None # remember feature scaling for future\n",
    "        self.imputer_dict = {} # dictionary between columns and its imputer\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "        # Perform imputation.\n",
    "        if config['preprocessing']['imputing']['groupby_first']['feature_list']:\n",
    "            df = preprocessing.groupby_impute(df, config)\n",
    "\n",
    "        if len(self.imputer_dict) == 0:\n",
    "            self.imputer_dict = preprocessing.fit_imputer(df, config)\n",
    "        imputed_df = preprocessing.impute(self.imputer_dict, df)\n",
    "\n",
    "        # encoding must be done after imputation, otherwise NA value is treated as a unique category unintentionally\n",
    "        # Perform categorical encoding on specified variables\n",
    "        if self.feature_encoding is None:\n",
    "            self.feature_encoding = preprocessing.get_categorical_encoding(imputed_df, config)\n",
    "        encoded_df = preprocessing.encode_categorical_features(imputed_df, config, self.feature_encoding)\n",
    "\n",
    "        # Perform scaling\n",
    "        if self.feature_scaling is None:\n",
    "            try:\n",
    "                self.feature_scaling = preprocessing.fit_scaler(encoded_df, config) # TODO: now we need to remember the scaler hasn't been fitted on config['target']. Is this good?\n",
    "            except KeyError:\n",
    "                raise KeyError(f\"Target column {config['target']} is not in the dataframe's columns!\")\n",
    "        encoded_df = preprocessing.scale(encoded_df, self.feature_scaling, config)\n",
    "\n",
    "        return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform filtering on samples by thresholding against features \n",
    "class SampleFilterByFeatureThresholdContainer:\n",
    "    \"\"\"\n",
    "    Container for filtering operations on the datset to remove unwanted rows. The index is not changed, however.\n",
    "    \"\"\"\n",
    "    def __call__(self, df: pd.DataFrame, config: dict):\n",
    "        for col, key_fn in config['preprocessing']['filtering'].items():\n",
    "            prev_len = len(df)\n",
    "            df = df[key_fn(df[col])] # remove height = 0 as they are invalid\n",
    "            new_len = len(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features\n",
    "class FeatureRemovalContainer: \n",
    "    \"\"\"\n",
    "    Container for feature removal operations to remove unwanted features.\n",
    "    \"\"\"\n",
    "    def __call__(self, df: pd.DataFrame, config: dict):\n",
    "        # remove by setting in config\n",
    "        col_names = config['preprocessing']['feature_removal']['feature_list']\n",
    "        drop = config['preprocessing']['feature_removal']['drop']\n",
    "        if drop:\n",
    "            df = df.drop(columns=col_names)\n",
    "        else:\n",
    "            df = df[col_names].copy()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom config for each of 5 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_index = cleaned_added_df.groupby(['RoadID', 'Start'])\n",
    "train_df = cleaned_added_df[grouped_index.ngroup().isin(\n",
    "    np.random.choice(\n",
    "        range(grouped_index.ngroups), \n",
    "        size=int(0.8 * grouped_index.ngroups), \n",
    "        replace=False)\n",
    "    )]\n",
    "valid_df = cleaned_added_df.loc[list(set(cleaned_added_df.index) - set(train_df.index))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nzta_configs.final_config import CONFIG, FeatureAdder\n",
    "\n",
    "# Initialise class containers\n",
    "feature_preprocess = FeatureEncodeImputeNormalizeContainer()\n",
    "sample_fitler = SampleFilterByFeatureThresholdContainer()\n",
    "feature_removal = FeatureRemovalContainer()\n",
    "\n",
    "# Sequential processing.\n",
    "filtered_df = sample_fitler(train_df, CONFIG)\n",
    "col_filtered_df = feature_removal(filtered_df, CONFIG)\n",
    "complete_df = feature_preprocess(col_filtered_df, CONFIG)\n",
    "\n",
    "# drop index now that we have performed imputed groupby\n",
    "display(complete_df)\n",
    "\n",
    "# Saving completed dataset\n",
    "util.save_complete_data(complete_df, flag=True, save_path=DATA_DIR / 'interim' / DATASET /  ('train_processed' + (f'_{experiment_suffix}' if experiment_suffix else '') + '.csv'), save_method='save_csv')\n",
    "\n",
    "# Saving preprocessing states for use on validation datasets\n",
    "state_dict = {\n",
    "    'config': CONFIG,\n",
    "    'feature_encoder': feature_preprocess.feature_encoding,\n",
    "    'scaler': feature_preprocess.feature_scaling,\n",
    "    'imputer_dict': feature_preprocess.imputer_dict\n",
    "}\n",
    "util.pickle_data(state_dict, CONFIG['preprocessing']['state_save_path'], f'preprocessing_state_dict' + (f'_{experiment_suffix}' if experiment_suffix else '') + '.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fitted preprocessing on valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(CONFIG['preprocessing']['state_save_path'] / f'preprocessing_state_dict{(\"_\" + experiment_suffix) if experiment_suffix else \"\"}.sav', 'rb') as f:\n",
    "    saved_state_dict = pickle.load(f)\n",
    "\n",
    "filtered_valid_df = sample_fitler(valid_df, CONFIG)\n",
    "col_filtered_valid_df = feature_removal(filtered_valid_df, CONFIG)\n",
    "groupby_impute_valid_df = preprocessing.groupby_impute(col_filtered_valid_df, CONFIG)\n",
    "imputed_valid_df = preprocessing.impute(saved_state_dict['imputer_dict'], groupby_impute_valid_df)\n",
    "encoded_valid_df = preprocessing.encode_categorical_features(imputed_valid_df, CONFIG, saved_state_dict['feature_encoder'])\n",
    "normalized_valid_df = preprocessing.scale(encoded_valid_df, saved_state_dict['scaler'], CONFIG)\n",
    "\n",
    "encoded_valid_df.to_csv(DATA_DIR / 'interim' / DATASET / 'encoded_valid.csv', index=False)\n",
    "util.save_complete_data(normalized_valid_df, flag=True, save_path=DATA_DIR / 'interim' / DATASET / f'valid_processed{(\"_\" + experiment_suffix) if experiment_suffix else \"\"}.csv', save_method='save_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply MRWA preprocessing on NZTA dataset. Can only done after running MRWA preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mrwa_configs.final_config import CONFIG as MRWA_CONFIG\n",
    "\n",
    "with open(MRWA_CONFIG['preprocessing']['state_save_path'] / (f'preprocessing_state_dict_{experiment_suffix.replace(\"nzta\", \"mrwa\")}.sav'), 'rb') as f:\n",
    "    saved_state_dict = pickle.load(f)\n",
    "\n",
    "filtered_valid_df = sample_fitler(valid_df, CONFIG)\n",
    "col_filtered_valid_df = feature_removal(filtered_valid_df, CONFIG)\n",
    "groupby_impute_valid_df = preprocessing.groupby_impute(col_filtered_valid_df, CONFIG)\n",
    "imputed_valid_df = preprocessing.impute(saved_state_dict['imputer_dict'], groupby_impute_valid_df)\n",
    "encoded_valid_df = preprocessing.encode_categorical_features(imputed_valid_df, CONFIG, saved_state_dict['feature_encoder'])\n",
    "MRWA_CONFIG['preprocessing']['normalizing']['feature_list'].remove('Direction')\n",
    "normalized_valid_df = preprocessing.scale(encoded_valid_df, saved_state_dict['scaler'], MRWA_CONFIG) # use mrwa config here since nz config does not contain unique one hot encode from wa\n",
    "\n",
    "encoded_valid_df.to_csv(DATA_DIR / 'interim' / DATASET / 'encoded_valid_transfer.csv', index=False)\n",
    "util.save_complete_data(normalized_valid_df, flag=True, save_path=DATA_DIR / 'interim' / DATASET / (f'valid_processed_{experiment_suffix}_TRANSFERED.csv'), save_method='save_csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
