{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook preprocesses a dataset through the following steps in order:\n",
    "\n",
    "- Apply transformations \n",
    "- Adding new columns\n",
    "- Filtering the dataframe\n",
    "- Dropping unused columns\n",
    "\n",
    "As detailed in the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.util as util\n",
    "import src.features.preprocessing as preprocessing \n",
    "\n",
    "from src.mrwa_configs.final_config import CONFIG, FeatureAdder\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from src import DATA_DIR\n",
    "\n",
    "DATASET = 'MRWA'\n",
    "all_df = util.load_data(**CONFIG['data']['preprocessing'])\n",
    "\n",
    "experiment_suffix = f'mrwa_final'\n",
    "\n",
    "# renaming\n",
    "all_df = all_df.rename(columns={\n",
    "    'Road': 'RoadID',\n",
    "    'Start Chainage': 'Start',\n",
    "    'Cway': 'Direction'\n",
    "})\n",
    "\n",
    "# convert km to m\n",
    "all_df.loc[:, 'Start'] = all_df['Start'] * 1000\n",
    "all_df.loc[:, 'End'] = all_df['Start'] + all_df['Length'] * 1000\n",
    "all_df = all_df.drop(columns=['Length'])\n",
    "\n",
    "# drop nas with index\n",
    "all_df = all_df.dropna(subset=['RoadID', 'Direction', 'Start', 'End', 'Date of condition data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_df = FeatureAdder()(all_df)\n",
    "display(added_df.head(1).drop([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_index = added_df.groupby(['RoadID', 'Direction', 'Start'])\n",
    "train_df = added_df[grouped_index.ngroup().isin(\n",
    "    np.random.choice(\n",
    "        range(grouped_index.ngroups), \n",
    "        size=int(0.8 * grouped_index.ngroups), \n",
    "        replace=False)\n",
    "    )]\n",
    "valid_df = added_df.loc[list(set(added_df.index) - set(train_df.index))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncodeImputeNormalizeContainer:\n",
    "    \"\"\"\n",
    "    Container for encoding, imputation, and normalization operations, in that order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_encoding = None # remember feature encoding for future\n",
    "        self.date_encoding = None # remember date encoding\n",
    "        self.feature_scaling = None # remember feature scaling for future\n",
    "        self.imputer_dict = {} # dictionary between columns and its imputer\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "        # Perform imputation.\n",
    "        if config['preprocessing']['imputing']['groupby_first']['feature_list']:\n",
    "            df = preprocessing.groupby_impute(df, config)\n",
    "\n",
    "        if len(self.imputer_dict) == 0:\n",
    "            self.imputer_dict = preprocessing.fit_imputer(df, config)\n",
    "        imputed_df = preprocessing.impute(self.imputer_dict, df)\n",
    "\n",
    "        # encoding must be done after imputation, otherwise NA value is treated as a unique category unintentionally\n",
    "        # Perform categorical encoding on specified variables\n",
    "        if self.feature_encoding is None:\n",
    "            self.feature_encoding = preprocessing.get_categorical_encoding(imputed_df, config)\n",
    "        encoded_df = preprocessing.encode_categorical_features(imputed_df, config, self.feature_encoding)\n",
    "\n",
    "        # Perform scaling\n",
    "        if self.feature_scaling is None:\n",
    "            try:\n",
    "                self.feature_scaling = preprocessing.fit_scaler(encoded_df, config) # TODO: now we need to remember the scaler hasn't been fitted on config['target']. Is this good?\n",
    "            except KeyError:\n",
    "                raise KeyError(f\"Target column {config['target']} is not in the dataframe's columns!\")\n",
    "        encoded_df = preprocessing.scale(encoded_df, self.feature_scaling, config)\n",
    "\n",
    "        return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform filtering on samples by thresholding against features \n",
    "class SampleFilterByFeatureThresholdContainer:\n",
    "    \"\"\"\n",
    "    Container for filtering operations on the datset to remove unwanted rows. The index is not changed, however.\n",
    "    \"\"\"\n",
    "    def __call__(self, df: pd.DataFrame, config: dict):\n",
    "        for col, key_fn in config['preprocessing']['filtering'].items():\n",
    "            prev_len = len(df)\n",
    "            df = df[key_fn(df[col])] # remove height = 0 as they are invalid\n",
    "            new_len = len(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features\n",
    "class FeatureRemovalContainer: \n",
    "    \"\"\"\n",
    "    Container for feature removal operations to remove unwanted features.\n",
    "    \"\"\"\n",
    "    def __call__(self, df: pd.DataFrame, config: dict):\n",
    "        # remove by setting in config\n",
    "        col_names = config['preprocessing']['feature_removal']['feature_list']\n",
    "        drop = config['preprocessing']['feature_removal']['drop']\n",
    "        if drop:\n",
    "            df = df.drop(columns=col_names)\n",
    "        else:\n",
    "            df = df[col_names].copy()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise class containers\n",
    "feature_preprocess = FeatureEncodeImputeNormalizeContainer()\n",
    "sample_fitler = SampleFilterByFeatureThresholdContainer()\n",
    "feature_removal = FeatureRemovalContainer()\n",
    "\n",
    "# Sequential processing.\n",
    "filtered_df = sample_fitler(train_df, CONFIG)\n",
    "col_filtered_df = feature_removal(filtered_df, CONFIG)\n",
    "complete_df = feature_preprocess(col_filtered_df, CONFIG)\n",
    "\n",
    "# drop index now that we have performed imputed groupby\n",
    "display(complete_df.head(1).drop(0))\n",
    "\n",
    "# Saving completed dataset\n",
    "util.save_complete_data(complete_df, flag=True, save_path=DATA_DIR / 'interim' / DATASET /  ('train_processed' + (f'_{experiment_suffix}' if experiment_suffix else '') + '.csv'), save_method='save_csv')\n",
    "\n",
    "# Saving preprocessing states for use on validation datasets\n",
    "state_dict = {\n",
    "    'config': CONFIG,\n",
    "    'feature_encoder': feature_preprocess.feature_encoding,\n",
    "    'scaler': feature_preprocess.feature_scaling,\n",
    "    'imputer_dict': feature_preprocess.imputer_dict\n",
    "}\n",
    "util.pickle_data(state_dict, CONFIG['preprocessing']['state_save_path'], f'preprocessing_state_dict' + (f'_{experiment_suffix}' if experiment_suffix else '') + '.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fitted preprocessing on valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(CONFIG['preprocessing']['state_save_path'] / ('preprocessing_state_dict' + (f'_{experiment_suffix}' if experiment_suffix else '') + '.sav'), 'rb') as f:\n",
    "    saved_state_dict = pickle.load(f)\n",
    "\n",
    "filtered_valid_df = sample_fitler(valid_df, CONFIG)\n",
    "col_filtered_valid_df = feature_removal(filtered_valid_df, CONFIG)\n",
    "groupby_impute_valid_df = preprocessing.groupby_impute(col_filtered_valid_df, CONFIG)\n",
    "imputed_valid_df = preprocessing.impute(saved_state_dict['imputer_dict'], col_filtered_valid_df)\n",
    "encoded_valid_df = preprocessing.encode_categorical_features(imputed_valid_df, CONFIG, saved_state_dict['feature_encoder'])\n",
    "normalized_valid_df = preprocessing.scale(encoded_valid_df, saved_state_dict['scaler'], CONFIG)\n",
    "\n",
    "encoded_valid_df.to_csv(DATA_DIR / 'interim' / DATASET / 'encoded_valid.csv', index=False)\n",
    "util.save_complete_data(normalized_valid_df, flag=True, save_path=DATA_DIR / 'interim' / DATASET / ('valid_processed' + (f'_{experiment_suffix}' if experiment_suffix else '') + '.csv'), save_method='save_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply mrwa preprocessing to nzta dataset. Can only be run after NZTA preprocessing is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nzta_configs.final_config import CONFIG as NZTA_CONFIG\n",
    "\n",
    "with open(NZTA_CONFIG['preprocessing']['state_save_path'] / (f'preprocessing_state_dict_{experiment_suffix.replace(\"mrwa\", \"nzta\")}.sav'), 'rb') as f:\n",
    "    saved_state_dict = pickle.load(f)\n",
    "\n",
    "filtered_valid_df = sample_fitler(valid_df, NZTA_CONFIG)\n",
    "col_filtered_valid_df = feature_removal(filtered_valid_df, CONFIG) # cannot use NZTA CONFIG here because we want to keep our direction column\n",
    "groupby_impute_valid_df = preprocessing.groupby_impute(col_filtered_valid_df, CONFIG) # cannot use NZ config here since group by condition is different\n",
    "imputed_valid_df = preprocessing.impute(saved_state_dict['imputer_dict'], groupby_impute_valid_df)\n",
    "encoded_valid_df = preprocessing.encode_categorical_features(imputed_valid_df, NZTA_CONFIG, saved_state_dict['feature_encoder'])\n",
    "normalized_valid_df = preprocessing.scale(encoded_valid_df, saved_state_dict['scaler'], NZTA_CONFIG)\n",
    "\n",
    "encoded_valid_df.to_csv(DATA_DIR / 'interim' / DATASET / 'encoded_valid_transfer.csv', index=False)\n",
    "util.save_complete_data(normalized_valid_df, flag=True, save_path=DATA_DIR / 'interim' / DATASET / (f'valid_processed_{experiment_suffix}_TRANSFERED.csv'), save_method='save_csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
