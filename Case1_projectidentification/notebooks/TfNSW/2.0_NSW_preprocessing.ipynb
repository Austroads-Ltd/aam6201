{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='../../img/WSP_red.png' style='height: 95px; float: left' alt='WSP Logo'/>\n",
    "<img src='../../img/austroads.png' style='height: 115px; float: right' alt='Client Logo'/>\n",
    "</div>\n",
    "<center><h2>AAM6201 Development of Machine-Learning Decision-Support tools for Pavement Asset Management<br>Case Study 1: Project Identification</h2></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command to autoreload changes in src\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.nsw_configs.final_config import DATA_DIR\n",
    "from tqdm.notebook import tqdm\n",
    "import src.util as util\n",
    "import src.features.preprocessing as preprocessing \n",
    "\n",
    "from src.nsw_configs.final_config import CONFIG, FeatureAdder\n",
    "import src.nsw_configs.final_config as config\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook preprocesses a dataset through the following steps in order:\n",
    "\n",
    "- Apply transformations \n",
    "- Adding new columns\n",
    "- Filtering the dataframe\n",
    "- Dropping unused columns\n",
    "\n",
    "As detailed in the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cracking = util.load_data(config.crack_data)\n",
    "dtims_in = util.load_data(config.dtims_in_data)\n",
    "dtims_out = util.load_data(config.dtims_out_data)\n",
    "profile = util.load_data(config.profile_data)\n",
    "deflection = util.load_data(config.deflection_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncodeImputeNormalizeContainer:\n",
    "    \"\"\"\n",
    "    Container for encoding, imputation, and normalization operations, in that order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_encoding = None # remember feature encoding for future\n",
    "        self.date_encoding = None # remember date encoding\n",
    "        self.feature_scaling = None # remember feature scaling for future\n",
    "        self.imputer_dict = {} # dictionary between columns and its imputer\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Perform imputation.\n",
    "        if CONFIG['preprocessing']['imputing']['groupby_first']['feature_list']:\n",
    "            df = preprocessing.groupby_impute(df, CONFIG)\n",
    "\n",
    "        if len(self.imputer_dict) == 0:\n",
    "            self.imputer_dict = preprocessing.fit_imputer(df, CONFIG)\n",
    "        imputed_df = preprocessing.impute(self.imputer_dict, df)\n",
    "\n",
    "        # encoding must be done after imputation, otherwise NA value is treated as a unique category unintentionally\n",
    "        # Perform categorical encoding on specified variables\n",
    "        if self.feature_encoding is None:\n",
    "            self.feature_encoding = preprocessing.get_categorical_encoding(imputed_df , CONFIG)\n",
    "        encoded_df = preprocessing.encode_categorical_features(imputed_df, CONFIG, self.feature_encoding)\n",
    "\n",
    "        # Perform scaling\n",
    "        if self.feature_scaling is None:\n",
    "            try:\n",
    "                self.feature_scaling = preprocessing.fit_scaler(encoded_df, CONFIG) # TODO: now we need to remember the scaler hasn't been fitted on CONFIG['target']. Is this good?\n",
    "            except KeyError:\n",
    "                raise KeyError(f\"Target column {CONFIG['target']} is not in the dataframe's columns!\")\n",
    "        encoded_df = preprocessing.scale(encoded_df, self.feature_scaling, CONFIG)\n",
    "\n",
    "        return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform filtering on samples by thresholding against features \n",
    "class SampleFilterByFeatureThresholdContainer:\n",
    "    \"\"\"\n",
    "    Container for filtering operations on the datset to remove unwanted rows. The index is not changed, however.\n",
    "    \"\"\"\n",
    "    def __call__(self, df: pd.DataFrame):\n",
    "        for col, key_fn in CONFIG['preprocessing']['filtering'].items():\n",
    "            df = df[key_fn(df[col])] # remove height = 0 as they are invalid\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features\n",
    "# TODO: Generalise this to accept configuration file\n",
    "class FeatureRemovalContainer: \n",
    "    \"\"\"\n",
    "    Container for feature removal operations to remove unwanted features.\n",
    "    \"\"\"\n",
    "    def __call__(self, df: pd.DataFrame):\n",
    "        # remove by setting in config\n",
    "        col_names = CONFIG['preprocessing']['feature_removal']['feature_list']\n",
    "        drop = CONFIG['preprocessing']['feature_removal']['drop']\n",
    "        if drop:\n",
    "            df = df.drop(columns=col_names)\n",
    "        else:\n",
    "            df = df[col_names].copy()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = \"AUSLINK_NETWORK DI_CLIMATE_ZONE DI_D0_STDEV DI_D1500 DI_D900 DI_DEFECT DI_IRI LGA_2 LINK_NO DI_NRM DI_PATCHES I_SPEED_TRUCK DI_SHLDWIDTH_LEFT DI_SHLDWIDTH_RIGHT I_AC_AGG_SIZE I_AC_BINDER I_AC_SPECIAL_TREATMENT I_AC_TYPE I_CONSISTENT_SURFACE_TYPE I_SS_CAT I_SS_COAT surf_function\".split()\n",
    "drop_cols = []\n",
    "df = dtims_in.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(age=2020-df[\"DI_YEAR_CONSTRUCTION\"])\n",
    "df = df.assign(age_surface=2020-df[\"DI_YEAR_SURF\"])\n",
    "df[\"COM_TRT\"] = df[\"COM_TRT\"].fillna(\"NoTreatment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = FeatureAdder()(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise class containers\n",
    "feature_preprocess = FeatureEncodeImputeNormalizeContainer()\n",
    "sample_fitler = SampleFilterByFeatureThresholdContainer()\n",
    "feature_removal = FeatureRemovalContainer()\n",
    "\n",
    "# Sequential processing.\n",
    "filtered_df = sample_fitler(df)\n",
    "col_filtered_df = feature_removal(filtered_df)\n",
    "complete_df = feature_preprocess(col_filtered_df)\n",
    "\n",
    "display(complete_df)\n",
    "\n",
    "# Saving completed dataset\n",
    "util.save_complete_data(complete_df, **CONFIG['preprocessing']['save_complete'])\n",
    "\n",
    "# Saving preprocessing states for use on validation datasets\n",
    "state_dict = {\n",
    "    'config': CONFIG,\n",
    "    'feature_encoder': feature_preprocess.feature_encoding,\n",
    "    'scaler': feature_preprocess.feature_scaling,\n",
    "    'imputer_dict': feature_preprocess.imputer_dict\n",
    "}\n",
    "util.pickle_data(state_dict, CONFIG['preprocessing']['state_save_path'], 'preprocessing_state_dict.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = dtims_out\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects[\"Treatment Date\"] = pd.to_datetime(projects[\"n_year\"], format=\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_lookup = util.load_data(DATA_DIR.parent / \"references\" / \"TreatmentCategory.csv\")\n",
    "treatment_lookup = treatment_lookup[treatment_lookup[\"Jurisdiction\"] == \"NSW\"]\n",
    "treatment_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape = projects.shape\n",
    "cleaned_projects = projects.dropna(\n",
    "    subset=[\"road\", \"ElementID_subseg_no\", \"n_year\", \"Trt\", \"Length_km\", \"program\", \"ElementID\"]\n",
    ").copy()\n",
    "\n",
    "old_shape = cleaned_projects.shape\n",
    "cleaned_projects = cleaned_projects[cleaned_projects[\"program\"] == \"dTIMS\"]\n",
    "\n",
    "cleaned_projects[\"Treatment Category\"] = cleaned_projects[\"Trt\"]\n",
    "cleaned_projects[\"Treatment Category\"] = cleaned_projects[\"Treatment Category\"].replace(dict(zip(treatment_lookup[\"Specific Category Value\"], treatment_lookup[\"Generic Category\"])))\n",
    "old_shape = cleaned_projects.shape\n",
    "cleaned_projects = cleaned_projects.drop(index=cleaned_projects[~cleaned_projects[\"Treatment Category\"].isin(treatment_lookup[\"Generic Category\"])].index)\n",
    "\n",
    "old_shape = cleaned_projects.shape\n",
    "cleaned_projects = cleaned_projects.drop(index=cleaned_projects[cleaned_projects[\"Treatment Category\"] == \"drop\"].index)\n",
    "\n",
    "cleaned_projects = cleaned_projects[[\"ElementID\", \"Treatment Date\", \"Treatment Category\"]]\n",
    "cleaned_projects = cleaned_projects.rename(columns={\"ElementID\": \"Road_Number\"})\n",
    "old_shape = cleaned_projects.shape\n",
    "cleaned_projects = cleaned_projects.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_projects.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = complete_df.rename(columns={\"ELEMENTID\": \"Road_Number\"})\n",
    "cleaned_df = complete_df\n",
    "cleaned_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_mat(grouped_labels: pd.DataFrame, treatments: list, latest_condition_date) -> pd.DataFrame:\n",
    "    label_mat = pd.DataFrame(columns=treatments, index=[\n",
    "        'Treatment within 1 year',\n",
    "        'Treatment between 1 to 3 years',\n",
    "        'Treatment between 3 to 5 years',\n",
    "        'Treatment between 5 to 10 years',\n",
    "        'Treatment between 10 to 30 years'\n",
    "    ])\n",
    "    label_mat.loc[:, :] = 0\n",
    "    if len(grouped_labels) == 0:\n",
    "        return label_mat\n",
    "\n",
    "    for i, treatment in enumerate(treatments):\n",
    "        category_labels = grouped_labels[grouped_labels['Treatment Category'] == treatment]\n",
    "        if len(category_labels) == 0:\n",
    "            continue\n",
    "        \n",
    "        year_offset = ((category_labels['Treatment Date'] - latest_condition_date) / np.timedelta64(1, 'Y'))\n",
    "\n",
    "        if (year_offset <= 1).any():\n",
    "            label_mat.iloc[0, i] = 1\n",
    "    \n",
    "        if ((year_offset > 1) & (year_offset <= 3)).any():\n",
    "            label_mat.iloc[1, i] = 1\n",
    "\n",
    "        if ((year_offset > 3) & (year_offset <= 5)).any():\n",
    "            label_mat.iloc[2, i] = 1\n",
    "\n",
    "        if ((year_offset > 5) & (year_offset <= 10)).any():\n",
    "            label_mat.iloc[3, i] = 1\n",
    "        \n",
    "        if ((year_offset > 10) & (year_offset <= 30)).any():\n",
    "            label_mat.iloc[4, i] = 1\n",
    "    \n",
    "    return label_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "treatments = cleaned_projects['Treatment Category'].unique()\n",
    "min_date_planned = cleaned_projects['Treatment Date'].min()\n",
    "\n",
    "flattened_data = []\n",
    "flattened_projects = []\n",
    "flattened_idx = []\n",
    "discarded_count = 0\n",
    "\n",
    "for idx in tqdm(cleaned_df.index, desc=\"index\"):\n",
    "    cur_seg = cleaned_df.loc[idx]\n",
    "    road_id = cur_seg[\"Road_Number\"]\n",
    "    \n",
    "    # find all projects in that section\n",
    "    labels = cleaned_projects[cleaned_projects['Road_Number'] == road_id]\n",
    "\n",
    "    # flatten label groups\n",
    "    label_mat = make_label_mat(labels, treatments, min_date_planned)\n",
    "    label_mat = pd.melt(label_mat.reset_index(), id_vars='index').rename(columns={\n",
    "        'index': 'key_type',\n",
    "        'variable': 'Treatment Category',\n",
    "        'value': 'boolean'\n",
    "    }).set_index(['key_type', 'Treatment Category']).transpose()\n",
    "    label_mat['no_project_flag'] = 1 if (label_mat.values != 0).sum() == 0 else 0\n",
    "\n",
    "    # append train and labels\n",
    "    flattened_projects.append(label_mat)\n",
    "flattened_data = cleaned_df.drop(columns=\"Road_Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(flattened_data)\n",
    "train_labels = pd.concat(flattened_projects, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_project_mask = train_labels.reset_index()[\"no_project_flag\"] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "save_dir = Path(DATA_DIR / \"processed\" / \"NSW\" / \"final\")\n",
    "if save_dir.exists() is False:\n",
    "    save_dir.mkdir(parents=True)\n",
    "\n",
    "train_df.to_csv(save_dir / \"train_all.csv\", index=False)\n",
    "train_labels.to_csv(save_dir / \"labels_all.csv\", index=False)\n",
    "cleaned_df.to_csv(save_dir / \"cleaned_condition_data.csv\", index=False)\n",
    "cleaned_projects.to_csv(save_dir / \"cleaned_projects.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('austroads_taskA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8558eca60468214515578ef8dc9d1a3cd923df7ae0c7c3b68d36aadcc2987ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
